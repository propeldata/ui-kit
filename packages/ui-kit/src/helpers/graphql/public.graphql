"""
The Propel GraphQL API schema
"""
schema {
  query: Query
  mutation: Mutation
}

directive @oneOf on INPUT_OBJECT

"""
Represents an ISO 8601 date and time in UTC. For example, "2022-08-18T08:53:33Z".
"""
scalar DateTime

"""
All Propel resources, such as Applications and Metrics, have a unique identifier, or ID. Typically, they may also have a unique name, which is specified in the interface `Common`.
"""
interface Node {
  id: ID!
}

"""
The ID or unique name input.

If both ID and unique name are provided, the ID will take precedence.
"""
input idOrUniqueName {
  """
  The unique identifier of the object.
  """
  id: String
  """
  The unique name of the object.
  """
  uniqueName: String
}

"""
The failure response object.
"""
type FailureResponse {
  """
  The error that caused the failure.
  """
  error: Error!
}

"""
The error object.
"""
type Error {
  """
  The error code.
  """
  code: Int
  """
  The error message.
  """
  message: String!
}

"""
The Account object.
"""
type Account {
  """
  The Account's unique identifier.
  """
  id: ID!
}

"""
The Environments object.

Environments are independent and isolated Propel workspaces for development, staging (testing), and production workloads. Environments are hosted in a specific region, initially in us-east-2 only.
"""
type Environment {
  """
  The Environment's unique identifier.
  """
  id: ID!
}

"""
All Propel resources, such as Applications and Metrics, have a set of common properties, such as the Propel Account and Environment that they are associated with. They also have a unique ID, which is specified in the interface `Node`.

Environments are independent and isolated Propel workspaces for development, staging (testing), and production workloads.
"""
interface Common {
  """
  The resource's unique name.
  """
  uniqueName: String!
  """
  The resource's description.
  """
  description: String!
  """
  The resource's Account.
  """
  account: Account!
  """
  The resource's Environment.
  """
  environment: Environment!
  """
  The resource's creation date and time in UTC.
  """
  createdAt: DateTime!
  """
  The resource's last modification date and time in UTC.
  """
  modifiedAt: DateTime!
  """
  The resource's creator. It can be either a User ID, an Application ID, or "system" if it was created by Propel.
  """
  createdBy: String!
  """
  The resource's last modifier. It can be either a User ID, an Application ID, or "system" if it was modified by Propel.
  """
  modifiedBy: String!
}

"""
The page info object used for pagination.
"""
type PageInfo {
  """
  Points to the first item returned in the results. Used when paginating backward.
  """
  startCursor: String
  """
  Points to the last item returned in the results. Used when paginating forward.
  """
  endCursor: String
  """
  A boolean that indicates whether a next page of results exists. Can be used to display a "next page" button in user interfaces, for example.
  """
  hasNextPage: Boolean!
  """
  A boolean that indicates whether a previous page of results exists. Can be used to display a "previous page" button in user interfaces, for example.
  """
  hasPreviousPage: Boolean!
}

"""
A Propeller determines your Application's query processing power. The larger the Propeller, the faster the queries and the higher the cost. Every Propel Application (and therefore every set of API credentials) has a Propeller that determines the speed and cost of queries.

[Learn more about Data Sources](https://www.propeldata.com/docs/applications#propeller).
"""
enum Propeller {
  """
  Max records per second: 5,000,000 records per second
  """
  P1_X_SMALL
  """
  Max records per second: 25,000,000 records per second
  """
  P1_SMALL
  """
  Max records per second: 100,000,000 records per second
  """
  P1_MEDIUM
  """
  Max records per second: 250,000,000 records per second
  """
  P1_LARGE
  """
  Max records per second: 500,000,000 records per second
  """
  P1_X_LARGE
}

"""
The Application object.

Propel Applications represent the web or mobile app you are building. They provide the API credentials that allow your client- or server-side app to access the Propel API. The Application's Propeller determines the speed and cost of your Metric Queries.

[Learn more about Applications](https://www.propeldata.com/docs/applications).
"""
type Application implements Node & Common {
  """
  The Application's unique identifier.
  """
  id: ID!
  """
  The Application's unique name.
  """
  uniqueName: String!
  """
  The Application's description.
  """
  description: String!
  """
  The Application's Account.
  """
  account: Account!
  """
  The Application's Environment.
  """
  environment: Environment!
  """
  The Application's creation date and time in UTC.
  """
  createdAt: DateTime!
  """
  The Application's last modification date and time in UTC.
  """
  modifiedAt: DateTime!
  """
  The Application's creator. It can be either a User ID, an Application ID, or "system" if it was created by Propel.
  """
  createdBy: String!
  """
  The Application's last modifier. It can be either a User ID, an Application ID, or "system" if it was modified by Propel.
  """
  modifiedBy: String!
  """
  The Application's OAuth 2.0 client identifier.
  """
  clientId: String!
  """
  The Application's OAuth 2.0 client secret.
  """
  secret: String
  """
  The Application's Propeller.
  """
  propeller: Propeller!
  """
  The Application's OAuth 2.0 scopes.
  """
  scopes: [ApplicationScope!]!
  """
  A paginated list of Policies associated with the Application.
  """
  policies(first: Int, after: String, last: Int, before: String): PolicyConnection!
    @deprecated(reason: "Use Data Pool Access Policies instead")
  """
  A paginated list of Data Pool Access Policies associated with the Application.
  """
  dataPoolAccessPolicies(first: Int, after: String, last: Int, before: String): DataPoolAccessPolicyConnection!
}

"""
The API operations an Application is authorized to perform.
"""
enum ApplicationScope {
  """
  Grant read/write access to Data Sources, Data Pools, Metrics and Policies.
  """
  ADMIN
  """
  Grant read/write access to Applications.
  """
  APPLICATION_ADMIN
  """
  Grant read access to query Data Pools.
  """
  DATA_POOL_QUERY
  """
  Grant read access to read Data Pools.
  """
  DATA_POOL_READ
  """
  Grant read access to fetch column statistics from Data Pools.
  """
  DATA_POOL_STATS
  """
  Grant read access to query Metrics.
  """
  METRIC_QUERY
  """
  Grant read access to fetch Dimension statistics from Metrics.
  """
  METRIC_STATS
  """
  Grant read access to Metrics.

  This does not allow querying Metrics. For that, see `METRIC_QUERY`.
  """
  METRIC_READ
}

"""
The fields for creating an Application.
"""
input createApplicationInput {
  """
  The Application's unique name. If not specified, Propel will set the ID as unique name.
  """
  uniqueName: String
  """
  The Application's description.
  """
  description: String
  """
  The Application's Propeller. If no Propeller is provided, Propel will set the Propeller to `P1_X_SMALL`.
  """
  propeller: Propeller
  """
  The Application's API authorization scopes. If specified, at least one scope must be provided; otherwise, all scopes will be granted to the Application by default.
  """
  scopes: [ApplicationScope!]
}

"""
The fields for modifying an Application.
"""
input modifyApplicationInput {
  """
  The ID or unique name of the Application to modify.
  """
  idOrUniqueName: idOrUniqueName!
  """
  The Application's new unique name.
  """
  uniqueName: String
  """
  The Application's new description.
  """
  description: String
  """
  The Application's new Propeller.
  """
  propeller: Propeller
  """
  The Application's new API authorization scopes.
  """
  scopes: [ApplicationScope!]
}

"""
The result of a mutation which creates or modifies an Application.

If successful, an `ApplicationResponse` will be returned; otherwise, a
`FailureResponse` will be returned.
"""
union ApplicationOrFailureResponse = ApplicationResponse | FailureResponse

"""
The result of a mutation which creates or modifies an Application.
"""
type ApplicationResponse {
  """
  The Application which was created or modified.
  """
  application: Application
}

"""
The Data Source object.

A Data Source is a connection to your data warehouse. It has the necessary connection details for Propel to access Snowflake or any other supported Data Source.

[Learn more about Data Sources](https://www.propeldata.com/docs/data-sources).
"""
type DataSource implements Node & Common {
  """
  The Data Source's unique identifier.
  """
  id: ID!
  """
  The Data Source's unique name.
  """
  uniqueName: String!
  """
  The Data Source's description.
  """
  description: String!
  """
  The Data Source's Account.
  """
  account: Account!
  """
  The Data Source's Environment.
  """
  environment: Environment!
  """
  The Data Source's creation date and time in UTC.
  """
  createdAt: DateTime!
  """
  The Data Source's last modification date and time in UTC.
  """
  modifiedAt: DateTime!
  """
  The Data Source's creator. It can be either a User ID, an Application ID, or "system" if it was created by Propel.
  """
  createdBy: String!
  """
  The Data Source's last modifier. It can be either a User ID, an Application ID, or "system" if it was modified by Propel.
  """
  modifiedBy: String!
  """
  The Data Source's type.
  """
  type: DataSourceType!
  """
  The Data Source's status.
  """
  status: DataSourceStatus!
  error: Error @deprecated(reason: "Refer to `checks` instead")
  """
  The Data Source's connection settings.
  """
  connectionSettings: ConnectionSettings!
  """
  The tables contained within the Data Source, according to the most recent table introspection.
  """
  tables(first: Int, after: String, last: Int, before: String): TableConnection
  """
  A list of table introspections performed for the Data Source. You can see how tables and columns changed over time by paging through this list.
  """
  tableIntrospections(first: Int, after: String, last: Int, before: String): TableIntrospectionConnection
  """
  A list of checks performed on the Data Source during its most recent connection attempt.
  """
  checks: [DataSourceCheck!]
  """
  If you list Data Pools via the `dataPools` field on a Data Source, you will get Data Pools for the Data Source.

  The `dataPools` field uses [cursor-based pagination](/docs/api/pagination) typical of GraphQL APIs. You can use the pairs of parameters `first` and `after` or `last` and `before` to page forward or backward through the results, respectively.

  For forward pagination, the `first` parameter defines the number of results to return, and the `after` parameter defines the cursor to continue from. You should pass the cursor for the _last_ result of the current page to `after`.

  For backward pagination, the `last` parameter defines the number of results to return, and the `before` parameter defines the cursor to continue from. You should pass the cursor for the _first_ result of the current page to `before`.
  """
  dataPools(first: Int, after: String, last: Int, before: String): DataPoolConnection
}

"""
The Data Source Check object.

Data Source Checks are executed when setting up your Data Source. They check that Propel will be able to receive data and setup Data Pools.

The exact Checks to perform vary by Data Source. For example, Snowflake-backed Data Sources will have their own specific Checks.
"""
type DataSourceCheck {
  """
  The name of the Data Source Check to be performed.
  """
  name: String!
  """
  A description of the Data Source Check to be performed.
  """
  description: String
  """
  The status of the Data Source Check (all checks begin as NOT_STARTED before transitioning to SUCCEEDED or FAILED).
  """
  status: DataSourceCheckStatus!
  """
  If the Data Source Check failed, this field includes a descriptive error message.
  """
  error: Error
  """
  The time at which the Data Source Check was performed.
  """
  checkedAt: DateTime
}

"""
The status of a Data Source Check.
"""
enum DataSourceCheckStatus {
  """
  The Check has not started.
  """
  NOT_STARTED
  """
  The Check succeeded.
  """
  SUCCEEDED
  """
  The Check failed.
  """
  FAILED
}

"""
The table introspection object.

When setting up a Data Source, Propel may need to introspect tables in order to determine what tables and columns are available to create Data Pools from. The table introspection represents the lifecycle of this operation (whether it's in-progress, succeeded, or failed) and the resulting tables and columns. These will be captured as table and column objects, respectively.
"""
type TableIntrospection {
  """
  The Data Source the table introspection was performed for.
  """
  dataSource: DataSource!
  """
  The status of the table introspection.
  """
  status: TableIntrospectionStatus!
  """
  The table introspection's creation date and time in UTC.
  """
  createdAt: DateTime!
  """
  The table introspection's creator. It can be either a User ID, an Application ID, or "system" if it was created by Propel.
  """
  createdBy: String!
  """
  The table introspection's last modification date and time in UTC.
  """
  modifiedAt: DateTime!
  """
  The table introspection's last modifier. It can be either a User ID, an Application ID, or "system" if it was modified by Propel.
  """
  modifiedBy: String!
  """
  The number of tables introspected.
  """
  numTables: Int
  """
  The tables introspected.
  """
  tables(first: Int, after: String, last: Int, before: String): TableConnection
}

"""
The status of a table introspection.
"""
enum TableIntrospectionStatus {
  """
  The table introspection has not started.
  """
  NOT_STARTED
  """
  The table introspection has started.
  """
  STARTED
  """
  The table introspection succeeded.
  """
  SUCCEEDED
  """
  The table introspection failed.
  """
  FAILED
}

"""
The table object.

Once a table introspection succeeds, it creates a new table object for every table it introspected.
"""
type Table {
  """
  The table's ID.
  """
  id: ID!
  """
  The table's name.
  """
  name: String!
  """
  The Data Source to which the table belongs.
  """
  dataSource: DataSource
  """
  The number of rows contained within the table at the time of introspection. Check the table's `cachedAt` time, since this info can become out of date.
  """
  rows: Int
  """
  The size of the table (in bytes) at the time of introspection. Check the table's `cachedAt` time, since this info can become out of date.
  """
  size: Int
  """
  Information about the table obtained from Snowflake.
  """
  kind: String! @deprecated(reason: "This is Snowflake-specific, and will be removed")
  """
  Information about the table obtained from Snowflake.
  """
  comment: String! @deprecated(reason: "This is Snowflake-specific, and will be removed")
  """
  Information about the table obtained from Snowflake.
  """
  clusterBy: String @deprecated(reason: "This is Snowflake-specific, and will be removed")
  """
  Information about the table obtained from Snowflake.
  """
  owner: String! @deprecated(reason: "This is Snowflake-specific, and will be removed")
  """
  Information about the table obtained from Snowflake.
  """
  timeTravelRetentionInDays: Int @deprecated(reason: "This is Snowflake-specific, and will be removed")
  """
  Information about the table obtained from Snowflake.
  """
  isAutomaticClusteringEnabled: Boolean @deprecated(reason: "This is Snowflake-specific, and will be removed")
  """
  Information about the table obtained from Snowflake.
  """
  isChangeTrackingEnabled: Boolean @deprecated(reason: "This is Snowflake-specific, and will be removed")
  """
  Information about the table obtained from Snowflake.
  """
  isSearchOptimizationEnabled: Boolean @deprecated(reason: "This is Snowflake-specific, and will be removed")
  """
  Information about the table obtained from Snowflake.
  """
  isExternal: Boolean! @deprecated(reason: "This is Snowflake-specific, and will be removed")
  """
  The time at which the table was cached (i.e., the time at which it was introspected).
  """
  cachedAt: DateTime!
  """
  The time at which the table was created. This is the same as its `cachedAt` time.
  """
  createdAt: DateTime!
  """
  The table's creator. This corresponds to the initiator of the table Introspection. It can be either a User ID, an Application ID, or "system" if it was created by Propel.
  """
  createdBy: String!
  """
  The table's columns.
  """
  columns(first: Int, after: String, last: Int, before: String): ColumnConnection
  """
  The table's columns which can be used as a timestamp for a Data Pool.
  """
  availableTimestamps(first: Int, after: String, last: Int, before: String): ColumnConnection
  """
  The table's columns which can be used as a measure for a Metric.
  """
  availableMeasures(first: Int, after: String, last: Int, before: String): ColumnConnection
}

"""
The column object.

Once a table introspection succeeds, it creates a new table object for every table it introspected. Within each table object, it also creates a column object for every column it introspected.
"""
type Column {
  """
  The column's name.
  """
  name: String!
  """
  The column's type.
  """
  type: String!
  """
  Information about the column obtained from Snowflake.
  """
  kind: String! @deprecated(reason: "This is Snowflake-specific, and will be removed")
  """
  Whether the column is nullable, meaning whether it accepts a null value.
  """
  isNullable: Boolean
  """
  Information about the column obtained from Snowflake.
  """
  defaultValue: String @deprecated(reason: "This is Snowflake-specific, and will be removed")
  """
  Information about the column obtained from Snowflake.
  """
  isPrimaryKey: Boolean @deprecated(reason: "This is Snowflake-specific, and will be removed")
  """
  Information about the column obtained from Snowflake.
  """
  isUniqueKey: Boolean @deprecated(reason: "This is Snowflake-specific, and will be removed")
  """
  Information about the column obtained from Snowflake.
  """
  comment: String @deprecated(reason: "This is Snowflake-specific, and will be removed")
  """
  Information about the column obtained from Snowflake.
  """
  policyName: String @deprecated(reason: "This is Snowflake-specific, and will be removed")
  """
  The time at which the column was cached (i.e., the time at which it was introspected).
  """
  cachedAt: DateTime!
  """
  The time at which the column was created. This is the same as its `cachedAt` time.
  """
  createdAt: DateTime!
  """
  The column's creator. This corresponds to the initiator of the table introspection. It can be either a User ID, an Application ID, or "system" if it was created by Propel.
  """
  createdBy: String!
  """
  This is the suggested Data Pool column type to use when converting this Data Source column to a Data Pool column.
  Propel makes this suggestion based on the Data Source column type. If the Data Source column type is unsupported, this field returns `null`.

  Sometimes, you know better which Data Pool column type to convert to. In these cases, you can refer
  to `supportedDataPoolColumnTypes` for the full set of supported conversions.
  """
  suggestedDataPoolColumnType: ColumnType
  """
  This is the set of supported Data Pool column types you can use when converting this Data Source column to a Data Pool column. If the Data Source column type is unsupported, this field returns an empty array.

  For example, a numeric Data Source column type could be converted to a narrower or wider numeric Data Pool column type; a string-valued Data Source column type could be mapped to a date or timestamp Data Pool column type.

  To learn more about the supported conversions, refer to [the docs](https://www.propeldata.com/docs) for your particular Data Source.
  """
  supportedDataPoolColumnTypes: [ColumnType!]!
}

"""
The types of Data Sources.
"""
enum DataSourceType {
  """
  Indicates a Webhook Data Source.
  """
  WEBHOOK
  """
  Indicates an S3 Data Source.
  """
  S3
  """
  Indicates a Redshift Data Source.
  """
  Redshift
  """
  Indicates an Http Data Source.
  """
  Http
  """
  Indicates a ClickHouse Data Source.
  """
  CLICKHOUSE
  """
  Indicates a BigQuery Data Source.
  """
  BIGQUERY
  """
  Indicates a Snowflake Data Source.
  """
  Snowflake
}

"""
The status of a Data Source.
"""
enum DataSourceStatus {
  """
  The Data Source has been created, but it is not connected yet.
  """
  CREATED
  """
  Propel is attempting to connect the Data Source.
  """
  CONNECTING
  """
  The Data Source is connected.
  """
  CONNECTED
  """
  The Data Source failed to connect.
  """
  BROKEN
  """
  Propel is deleting the Data Source.
  """
  DELETING
}

union ConnectionSettings =
    SnowflakeConnectionSettings
  | HttpConnectionSettings
  | S3ConnectionSettings
  | WebhookConnectionSettings

"""
The Snowflake Data Source connection settings.
"""
type SnowflakeConnectionSettings {
  """
  The Snowflake account. This is the part before the "snowflakecomputing.com" part of your Snowflake URL.
  """
  account: String!
  """
  The Snowflake database name.
  """
  database: String!
  """
  The Snowflake warehouse name. It should be "PROPELLING" if you used the default name in the setup script.
  """
  warehouse: String!
  """
  The Snowflake schema.
  """
  schema: String!
  """
  The Snowflake username. It should be "PROPEL" if you used the default name in the setup script.
  """
  username: String!
  """
  The Snowflake role. It should be "PROPELLER" if you used the default name in the setup script.
  """
  role: String!
}

"""
The HTTP Basic authentication settings.
"""
type HttpBasicAuthSettings {
  """
  Username for HTTP Basic authentication that must be included in the Authorization header when uploading new data.
  """
  username: String!
  """
  Password for HTTP Basic authentication that must be included in the Authorization header when uploading new data.
  """
  password: String!
}

"""
The fields for creating a Snowflake Data Source.
"""
input createSnowflakeDataSourceInput {
  """
  The Data Source's unique name. If not specified, Propel will set the ID as unique name.
  """
  uniqueName: String
  """
  The Data Source's description.
  """
  description: String
  """
  The Data Source's connection settings.
  """
  connectionSettings: SnowflakeConnectionSettingsInput!
}

"""
The fields for modifying a Snowflake Data Source.
"""
input modifySnowflakeDataSourceInput {
  """
  The ID or unique name of the Data Source to modify.
  """
  idOrUniqueName: idOrUniqueName!
  """
  The Data Source's new unique name.
  """
  uniqueName: String
  """
  The Data Source's new description.
  """
  description: String
  """
  The Data Source's new connection settings.
  """
  connectionSettings: PartialSnowflakeConnectionSettingsInput
}

"""
The fields for creating a Snowflake Data Source's connection settings.
"""
input SnowflakeConnectionSettingsInput {
  """
  The Snowflake account. Only include the part before the "snowflakecomputing.com" part of your Snowflake URL (make sure you are in classic console, not Snowsight). For AWS-based accounts, this looks like "znXXXXX.us-east-2.aws". For Google Cloud-based accounts, this looks like "ffXXXXX.us-central1.gcp".
  """
  account: String!
  """
  The Snowflake database name.
  """
  database: String!
  """
  The Snowflake warehouse name. It should be "PROPELLING" if you used the default name in the setup script.
  """
  warehouse: String!
  """
  The Snowflake schema.
  """
  schema: String!
  """
  The Snowflake username. It should be "PROPEL" if you used the default name in the setup script.
  """
  username: String!
  """
  The Snowflake password.
  """
  password: String!
  """
  The Snowflake role. It should be "PROPELLER" if you used the default name in the setup script.
  """
  role: String!
}

"""
The fields for modifying a Snowflake Data Source's connection settings.
"""
input PartialSnowflakeConnectionSettingsInput {
  """
  The Snowflake account. Only include the part before the "snowflakecomputing.com" part of your Snowflake URL (make sure you are in classic console, not Snowsight). For AWS-based accounts, this looks like "znXXXXX.us-east-2.aws". For Google Cloud-based accounts, this looks like "ffXXXXX.us-central1.gcp". If not provided this property will not be modified.
  """
  account: String
  """
  The Snowflake database name. If not provided this property will not be modified.
  """
  database: String
  """
  The Snowflake warehouse name. It should be "PROPELLING" if you used the default name in the setup script. If not provided this property will not be modified.
  """
  warehouse: String
  """
  The Snowflake schema. If not provided this property will not be modified.
  """
  schema: String
  """
  The Snowflake username. It should be "PROPEL" if you used the default name in the setup script. If not provided this property will not be modified.
  """
  username: String
  """
  The Snowflake password. If not provided this property will not be modified.
  """
  password: String
  """
  The Snowflake role. It should be "PROPELLER" if you used the default name in the setup script. If not provided this property will not be modified.
  """
  role: String
}

"""
The Propel data types.
"""
enum ColumnType {
  """
  True or false.
  """
  BOOLEAN
  """
  A variable-length string.
  """
  STRING
  """
  A 32-bit signed double-precision floating point number.
  """
  FLOAT
  """
  A 64-bit signed double-precision floating point number.
  """
  DOUBLE
  """
  An 8-bit signed integer, with a minimum value of -2⁷ and a maximum value of 2⁷-1.
  """
  INT8
  """
  A 16-bit signed integer, with a minimum value of -2¹⁵ and a maximum value of 2¹⁵-1.
  """
  INT16
  """
  A 32-bit signed integer, with a minimum value of -2³¹ and a maximum value of 2³¹-1.
  """
  INT32
  """
  A 64-bit signed integer, with a minimum value of -2⁶³ and a maximum value of 2⁶³-1.
  """
  INT64
  """
  A date without a timestamp. For example, "YYYY-MM-DD".
  """
  DATE
  """
  A date with a timestamp. For example, "yyy-MM-dd HH:mm:ss".
  """
  TIMESTAMP
  """
  A JavaScript Object Notation (JSON) document.
  """
  JSON
}

"""
The fields for specifying an HTTP Data Source's Basic authentication settings.
"""
input HttpBasicAuthInput {
  """
  The username for HTTP Basic authentication that must be included in the Authorization header when uploading new data.
  """
  username: String!
  """
  The password for HTTP Basic authentication that must be included in the Authorization header when uploading new data.
  """
  password: String!
}

"""
An HTTP Data Source's table.
"""
type HttpDataSourceTable {
  """
  The ID of the table
  """
  id: ID!
  """
  The name of the table
  """
  name: String!
  """
  All the columns present in the table
  """
  columns: [HttpDataSourceColumn!]!
}

"""
The fields for specifying an HTTP Data Source's table.
"""
input HttpDataSourceTableInput {
  """
  The name of the table
  """
  name: String!
  """
  All the columns present in the table
  """
  columns: [HttpDataSourceColumnInput!]!
}

"""
A column in an HTTP Data Source's table.
"""
type HttpDataSourceColumn {
  """
  The column name. It has to be unique within a Table.
  """
  name: String!
  """
  The column type.
  """
  type: ColumnType!
  """
  Whether the column's type is nullable or not.
  """
  nullable: Boolean!
}

"""
The fields for specifying a column in an HTTP Data Source's table.
"""
input HttpDataSourceColumnInput {
  """
  The column name. It has to be unique within a Table.
  """
  name: String!
  """
  The column type.
  """
  type: ColumnType!
  """
  Whether the column's type is nullable or not.
  """
  nullable: Boolean!
}

"""
An S3 Data Source's table.
"""
type S3DataSourceTable {
  """
  The ID of the table
  """
  id: ID!
  """
  The name of the table
  """
  name: String!
  """
  The path to the table's files in S3.
  """
  path: String
  """
  All the columns present in the table
  """
  columns: [S3DataSourceColumn!]!
}

"""
The fields for specifying an S3 Data Source's table.
"""
input S3DataSourceTableInput {
  """
  The name of the table
  """
  name: String!
  """
  The path to the table's files in S3.
  """
  path: String
  """
  All the columns present in the table
  """
  columns: [S3DataSourceColumnInput!]!
}

"""
A column in an S3 Data Source's table.
"""
type S3DataSourceColumn {
  """
  The column name.
  """
  name: String!
  """
  The column type.
  """
  type: ColumnType!
  """
  Whether the column's type is nullable or not.
  """
  nullable: Boolean!
}

"""
The fields for specifying a column in an S3 Data Source's table.
"""
input S3DataSourceColumnInput {
  """
  The column name. It has to be unique within a Table.
  """
  name: String!
  """
  The column type.
  """
  type: ColumnType!
  """
  Whether the column's type is nullable or not.
  """
  nullable: Boolean!
}

"""
A column in the Webhook Data Source's table.
"""
type WebhookDataSourceColumn {
  "The column name."
  name: String!
  """
  The JSON property that the column will be derived from. For example, if you POST a JSON event like this:
  ```json
  { "greeting": { "message": "hello, world" } }
  ```
  Then you can use the JSON property "greeting.message" to extract "hello, world" to a column.
  """
  jsonProperty: String!
  "The column type."
  type: ColumnType!
  "Whether the column's type is nullable or not."
  nullable: Boolean!
}

"""
The fields for specifying a column in an Webhook Data Source's table.
"""
input WebhookDataSourceColumnInput {
  "The column name. It has to be unique within a Table."
  name: String!
  """
  The JSON property that the column will be derived from. For example, if you POST a JSON event like this:
  ```json
  { "greeting": { "message": "hello, world" } }
  ```
  Then you can use the JSON property "greeting.message" to extract "hello, world" to a column.
  """
  jsonProperty: String!
  "The column type."
  type: ColumnType!
  "Whether the column's type is nullable or not."
  nullable: Boolean!
}

"""
The result of a mutation which creates or modifies a DataSource.

If successful, an `DataSourceResponse` will be returned; otherwise, a
`FailureResponse` will be returned.
"""
union DataSourceOrFailureResponse = DataSourceResponse | FailureResponse

"""
The result of a mutation which creates or modifies a Data Source.
"""
type DataSourceResponse {
  """
  The Data Source which was created or modified.
  """
  dataSource: DataSource
}

"""
The Data Pool object. Data Pools are Propel's high-speed data store and cache

[Learn more about Data Pools](https://www.propeldata.com/docs/connect-your-data#key-concept-2-data-pools).
"""
type DataPool implements Node & Common {
  "The Data Pool's unique identifier."
  id: ID!
  "The Data Pool's unique name."
  uniqueName: String!
  "The Data Pool's description."
  description: String!
  "The Data Pool's Account."
  account: Account!
  "The Data Pool's Environment."
  environment: Environment!
  "The Data Pool's creation date and time in UTC."
  createdAt: DateTime!
  "The Data Pool's last modification date and time in UTC."
  modifiedAt: DateTime!
  """
  The Data Pool's creator. It can be either a User ID, an Application ID, or "system" if it was created by Propel.
  """
  createdBy: String!
  """
  The Data Pool's last modifier. It can be either a User ID, an Application ID, or "system" if it was modified by Propel.
  """
  modifiedBy: String!
  "The Data Pool's Data Source."
  dataSource: DataSource!
  "The Data Pool's status."
  status: DataPoolStatus!
  error: Error @deprecated(reason: "Refer to `setupTasks` instead")
  "The Data Pool's data retention in days (not yet supported)."
  dataRetentionInDays: Int!
  "The name of the Data Pool's table."
  table: String!
  "The Data Pool's primary timestamp column."
  timestamp: Timestamp!
  "The Data Pool's tenant ID, if configured."
  tenant: Tenant @deprecated(reason: "Will be removed; use Data Pool Access Policies instead")
  "The Data Pool's unique ID column. Propel uses the primary timestamp and a unique ID to compose a primary key for determining whether records should be inserted, deleted, or updated within the Data Pool."
  uniqueId: UniqueId
  "The number of records in the Data Pool."
  recordCount: String
  "The amount of storage in terabytes used by the Data Pool."
  sizeInTerabytes: Float
  "The Data Pool's columns."
  columns(first: Int, after: String, last: Int, before: String): DataPoolColumnConnection
  "The list of measures (numeric columns) in the Data Pool."
  availableMeasures(first: Int, after: String, last: Int, before: String): DataPoolColumnConnection
  "A list of setup tasks performed on the Data Pool during its most recent setup attempt."
  setupTasks: [DataPoolSetupTask!]
  "Settings related to Data Pool syncing."
  syncing: DataPoolSyncing!
  "The list of Syncs of the Data Pool."
  syncs(filter: SyncsFilter, first: Int, after: String, last: Int, before: String): SyncConnection
  "The list of Metrics powered by the Data Pool."
  metrics(first: Int, after: String, last: Int, before: String): MetricConnection
  "The Deletion Jobs that were historically issued to this Data Pool, sorted by creation time, in descending order."
  deletionJobs(first: Int, after: String, last: Int, before: String): DeletionJobConnection
  "The Add Column Jobs that were historically issued to this Data Pool, sorted by creation time, in descending order."
  addColumnToDataPoolJobs(first: Int, after: String, last: Int, before: String): AddColumnToDataPoolJobConnection
  "The UpdateDataPoolRecords Jobs that were historically issued to this Data Pool, sorted by creation time, in descending order."
  updateDataPoolRecordsJobs(first: Int, after: String, last: Int, before: String): UpdateDataPoolRecordsJobConnection
  """
  Whether the Data Pool has access control enabled or not.

  If the Data Pool has access control enabled, Applications must be assigned Data Pool Access
  Policies in order to query the Data Pool and its Metrics.
  """
  accessControlEnabled: Boolean!
  "A paginated list of Data Pool Access Policies available on the Data Pool."
  dataPoolAccessPolicies(first: Int, after: String, last: Int, before: String): DataPoolAccessPolicyConnection!
  "Validates a custom expression against the Data Pool's available columns. If the provided expression is invalid, the ValidateExpressionResult response will contain a reason explaining why."
  validateExpression(expression: String!): ValidateExpressionResult!
}

"""
The Data Pool Setup Task object.

Data Pool Setup Tasks are executed when setting up your Data Pool. They ensure Propel will be able to sync records from your Data Source to your Data Pool.

The exact Setup Tasks to perform vary by Data Source. For example, Data Pools pointing to a Snowflake-backed Data Sources will have their own specific Setup Tasks.
"""
type DataPoolSetupTask {
  """
  The name of the Data Pool Setup Task to be performed.
  """
  name: String!
  """
  A description of the Data Pool Setup Task to be performed.
  """
  description: String
  """
  The status of the Data Pool Setup Task (all setup tasks begin as NOT_STARTED before transitioning to SUCCEEDED or FAILED).
  """
  status: DataPoolSetupTaskStatus!
  """
  If the Data Pool Setup Task failed, this field includes a descriptive error message.
  """
  error: Error
  """
  The time at which the Data Pool Setup Task was completed.
  """
  completedAt: DateTime
}

"""
The status of a Data Pool Setup Task.
"""
enum DataPoolSetupTaskStatus {
  """
  The Data Pool Setup Task has not been started yet.
  """
  NOT_STARTED
  """
  The Data Pool Setup Task has completed successfully.
  """
  SUCCEEDED
  """
  The Data Pool Setup Task has failed.
  """
  FAILED
}

"""
The Dimension object that represents a column in a table.
"""
type Dimension {
  """
  The column name it represents.
  """
  columnName: String!
  """
  The column data type.
  """
  type: String!
  """
  Whether the column is nullable.
  """
  isNullable: Boolean
  """
  Whether the column is a unique key.
  """
  isUniqueKey: Boolean @deprecated(reason: "This is Snowflake-specific, and will be removed")
  """
  The statistics for the dimension values. Fetching statistics incurs query costs.
  """
  stats: DimensionStatistics
}

"""
Statistics about a particular Dimension.
"""
type DimensionStatistics {
  """
  An array of unique values for the Dimension, up to 1,000. Empty if the Dimension contains more than 1,000 unique values. Fetching unique values incurs query costs.
  """
  uniqueValues(limit: Int): [String!]
  """
  The minimum value of the Dimension.
  """
  min: String
  """
  The maximum value of the Dimension.
  """
  max: String
  """
  The average value of the Dimension. Empty for non-numeric Dimensions.
  """
  average: String
  """
  The Query statistics and metadata.
  """
  query: QueryInfo!
}

"""
The fields of a filter.

You can construct more complex filters using `and` and `or`. For example, to construct a filter equivalent to

```
(value > 0 AND value <= 100) OR status = "confirmed"
```

you could write

```
{
  "column": "value",
  "operator": "GREATER_THAN",
  "value": "0",
  "and": [{
    "column": "value",
    "operator": "LESS_THAN_OR_EQUAL_TO",
    "value": "0"
  }],
  "or": [{
    "column": "status",
    "operator": "EQUALS",
    "value": "confirmed"
  }]
}
```

Note that `and` takes precedence over `or`.
"""
type Filter {
  """
  The name of the column to filter on.
  """
  column: String!
  """
  The operation to perform when comparing the column and filter values.
  """
  operator: FilterOperator!
  """
  The value to compare the column to.
  """
  value: String
  """
  Additional filters to AND with this one. AND takes precedence over OR.
  """
  and: [Filter!]
  """
  Additional filters to OR with this one. AND takes precedence over OR.
  """
  or: [Filter!]
}

"""
The fields for creating an Update Data Pool Records Job.

```
{
"column": "status",
"expression": "'completed'"
}

{
"column": "counter",
"expression": "counter + 1"
}

{
"column": "full_name",
"expression": "concat(first_name, ' ', last_name)"
}
```
"""
type UpdateDataPoolRecordsJobSetColumn {
  """
  The name of the column to update.
  """
  column: String!
  """
  The value to which the column will be updated. Once evaluated, it should be of the same data type as the column.
  """
  expression: String!
}

"""
The fields for creating or modifying a Dimension.
"""
input DimensionInput {
  """
  The name of the column to create the Dimension from.
  """
  columnName: String!
}

"""
The status of a Data Pool.
"""
enum DataPoolStatus {
  """
  The Data Pool has been created and will be set up soon.
  """
  CREATED
  """
  Propel is attempting to set up the Data Pool.
  """
  PENDING
  """
  The Data Pool is set up and serving data. Check its Syncs to monitor data ingestion.
  """
  LIVE
  """
  The Data Pool setup failed. Check its Setup Tasks before re-attempting setup.
  """
  SETUP_FAILED
  CONNECTING @deprecated(reason: "Start using `PENDING` instead")
  CONNECTED @deprecated(reason: "Start using `LIVE` instead")
  BROKEN @deprecated(reason: "Start using `SETUP_FAILED` instead")
  PAUSING @deprecated(reason: "The ability to pause will move to a new field")
  PAUSED @deprecated(reason: "The ability to pause will move to a new field")
  """
  Propel is deleting the Data Pool and all of its associated data.
  """
  DELETING
}

"""
A Data Pool's primary timestamp column. Propel uses the primary timestamp to order and partition your data in Data Pools. It will serve as the time dimension for your Metrics.
"""
type Timestamp {
  "The name of the column that represents the primary timestamp."
  columnName: String!
  "The primary timestamp column's type."
  type: String!
}

"""
The fields to specify the Data Pool's primary timestamp column. Propel uses the primary timestamp to order and partition your data in Data Pools. It will serve as the time dimension for your Metrics.
"""
input TimestampInput {
  "The name of the column that represents the primary timestamp."
  columnName: String!
}

"""
A Data Pool's tenant ID column. The tenant ID column is used to control access to your data with access policies.
"""
type Tenant {
  "The name of the column that represents the tenant ID."
  columnName: String!
  "The tenant ID column's type."
  type: String!
}

"""
The fields to specify the Data Pool's tenant ID column. The tenant ID column is used to control access to your data with access policies.
"""
input TenantInput {
  "The name of the column that represents the tenant ID."
  columnName: String!
}

"""
A Data Pool's unique ID column. Propel uses the primary timestamp and a unique ID to compose a primary key for determining whether records should be inserted, deleted, or updated within the Data Pool.
"""
type UniqueId {
  "The name of the column that represents the unique ID."
  columnName: String!
}

"""
The fields to specify the Data Pool's unique ID column. Propel uses the primary timestamp and a unique ID to compose a primary key for determining whether records should be inserted, deleted, or updated within the Data Pool.
"""
input UniqueIdInput {
  "The name of the column that represents the unique ID."
  columnName: String!
}

"""
The available Data Pool sync intervals. Specify unit of time between attempts to sync data from your data warehouse.

Note that the syncing interval is approximate. For example, setting the syncing interval to `EVERY_1_HOUR` does not mean that syncing will occur exactly on the hour. Instead, the syncing interval starts relative to when the Data Pool goes `LIVE`, and Propel will attempt to sync approximately every hour. Additionally, if you pause or resume syncing, this too can shift the syncing interval around.
"""
enum DataPoolSyncInterval {
  EVERY_1_MINUTE
  EVERY_5_MINUTES
  EVERY_15_MINUTES
  EVERY_30_MINUTES
  EVERY_1_HOUR
  EVERY_4_HOURS
  EVERY_12_HOURS
  EVERY_24_HOURS
}

type DataPoolColumn {
  """
  The name of the Data Source column that this Data Pool column derives from.
  """
  name: String @deprecated(reason: "Start using `columnName` instead")
  """
  The name of the Data Source column that this Data Pool column derives from.
  """
  columnName: String!
  """
  The Data Pool column's type. This may differ from the corresponding Data Source column's type.
  """
  type: ColumnType!
  """
  Whether the column is nullable, meaning whether it accepts a null value.
  """
  isNullable: Boolean!
}

input DataPoolColumnInput {
  """
  The name of the Data Source column that this Data Pool column derives from.
  """
  columnName: String!
  """
  The Data Pool column's type. This may differ from the corresponding Data Source column's type.
  """
  type: ColumnType!
  """
  Whether the column is nullable, meaning whether it accepts a null value.
  """
  isNullable: Boolean!
}

"""
The fields for creating a Data Pool.
"""
input CreateDataPoolInputV2 {
  "The Data Source that will be used to create the Data Pool."
  dataSource: ID!
  "The table that the Data Pool will sync from."
  table: String!
  "The table's primary timestamp column. Propel uses the primary timestamp to order and partition your data in Data Pools. It will serve as the time dimension for your Metrics."
  timestamp: TimestampInput!
  "The Data Pool's unique name. If not specified, Propel will set the ID as the unique name."
  uniqueName: String
  "The Data Pool's description."
  description: String
  "The list of columns."
  columns: [DataPoolColumnInput!]!
  "The Data Pool's optional tenant ID column. The tenant ID column is used to control access to your data with access policies."
  tenant: TenantInput @deprecated(reason: "Will be removed; use Data Pool Access Policies instead")
  """
  The Data Pool's unique ID column. Propel uses the primary timestamp and a unique ID to compose a primary key for determining whether records should be inserted, deleted, or updated within the Data Pool.
  """
  uniqueId: UniqueIdInput
  """
  The Data Pool's syncing settings.
  """
  syncing: DataPoolSyncingInput
  """
  Enables or disables access control for the Data Pool.

  If the Data Pool has access control enabled, Applications must be assigned Data Pool Access
  Policies in order to query the Data Pool and its Metrics.
  """
  accessControlEnabled: Boolean
}

"""
The fields for modifying a Data Pool.
"""
input modifyDataPoolInput {
  """
  The ID or unique name of the Data Pool to modify.
  """
  idOrUniqueName: idOrUniqueName!
  """
  The Data Pool's new unique name.
  """
  uniqueName: String
  """
  The Data Pool's new description.
  """
  description: String
  """
  The Data Pool's new data retention in days.
  """
  dataRetentionInDays: Int
  """
  The Data Pool's new syncing settings.
  """
  syncing: DataPoolSyncingInput
  """
  Enables or disables access control for the Data Pool.

  If the Data Pool has access control enabled, Applications must be assigned Data Pool Access
  Policies in order to query the Data Pool and its Metrics.
  """
  accessControlEnabled: Boolean
}

"""
The result of a mutation which creates or modifies a Data Pool.

If successful, an `DataPoolResponse` will be returned; otherwise, a
`FailureResponse` will be returned.
"""
union DataPoolOrFailureResponse = DataPoolResponse | FailureResponse

"""
The result of a mutation which creates or modifies a Data Pool.
"""
type DataPoolResponse {
  """
  The Data Pool which was created or modified.
  """
  dataPool: DataPool
}

"""
The Data Pool Sync Status. It indicates whether a Data Pool is syncing data or not.
"""
enum DataPoolSyncStatus {
  """
  Syncing is enabled for the Data Pool.
  """
  ENABLED
  """
  Propel is disabling syncing for the Data Pool.
  """
  DISABLING
  """
  Syncing is disabled for the Data Pool.
  """
  DISABLED
}

"""
Settings related to Data Pool syncing.
"""
type DataPoolSyncing {
  """
  Indicates whether syncing is enabled or disabled.
  """
  status: DataPoolSyncStatus!
  """
  The syncing interval.

  Note that the syncing interval is approximate. For example, setting the syncing interval to `EVERY_1_HOUR`
  does not mean that syncing will occur exactly on the hour. Instead, the syncing interval starts relative to
  when the Data Pool goes `LIVE`, and Propel will attempt to sync approximately every hour. Additionally,
  if you pause or resume syncing, this too can shift the syncing interval around.
  """
  interval: DataPoolSyncInterval
  """
  The date and time of the most recent Sync in UTC.
  """
  lastSyncedAt: DateTime
}

"""
The fields for modifying the Data Pool syncing.
"""
input DataPoolSyncingInput {
  interval: DataPoolSyncInterval!
}

"""
The Sync object.

This represents the process of syncing data from your Data Source (for example, a Snowflake data warehouse) to your Data Pool.
"""
type Sync implements Node {
  """
  The Sync's unique identifier.
  """
  id: ID!
  """
  The Sync's Account.
  """
  account: Account
  """
  The Sync's Environment.
  """
  environment: Environment
  """
  The Sync's creation date and time in UTC.
  """
  createdAt: DateTime!
  """
  The Sync's last modification date and time in UTC.
  """
  modifiedAt: DateTime!
  """
  The Sync's creator. It can be either a User ID, an Application ID, or "system" if it was created by Propel.
  """
  createdBy: String!
  """
  The Sync's last modifier. It can be either a User ID, an Application ID, or "system" if it was modified by Propel.
  """
  modifiedBy: String!
  """
  The Sync's Data Pool.
  """
  dataPool: DataPool
  """
  The Sync's Data Pool's Data Source.
  """
  dataSource: DataSource
  """
  The number of new, updated, and deleted records contained within the Sync, if known. This excludes filtered records.
  """
  processedRecords: String
  """
  The number of new records contained within the Sync, if known. This excludes filtered records.
  """
  newRecords: String
  """
  The number of updated records contained within the Sync, if known. This excludes filtered records.
  """
  updatedRecords: String
  """
  The number of deleted records contained within the Sync, if known. This excludes filtered records.
  """
  deletedRecords: String
  """
  The number of filtered records contained within the Sync, due to issues such as a missing timestamp Dimension, if any are known to be invalid.
  """
  invalidRecords: String
  """
  The (compressed) size of the Sync, in bytes, if known.
  """
  size: String
  """
  The status of the Sync (all Syncs begin as SYNCING before transitioning to SUCCEEDED or FAILED).
  """
  status: SyncStatus!
  """
  The time at which the Sync started.
  """
  startedAt: DateTime
  """
  The time at which the Sync succeeded.
  """
  succeededAt: DateTime
  """
  The time at which the Sync failed.
  """
  failedAt: DateTime
  """
  If the Sync failed, this represents the reason the Sync failed.
  """
  error: Error
}

"""
The status of a Sync.
"""
enum SyncStatus {
  """
  Propel is actively syncing records contained within the Sync.
  """
  SYNCING
  """
  The Sync succeeded. Propel successfully synced all records contained within the Sync.
  """
  SUCCEEDED
  """
  The Sync failed. Propel failed to sync some or all records contained within the Sync.
  """
  FAILED
  """
  Propel is deleting the Sync.
  """
  DELETING @deprecated(reason: "No longer used")
}

"""
The filter to apply when listing the Syncs for a Data Pool.
"""
enum SyncsFilter {
  """
  Returns only Syncs with empty records.
  """
  EMPTY
  """
  Returns only Syncs that contain one or more records.
  """
  NOT_EMPTY
  """
  Returns all Syncs, regardless of whether they contain records or not.
  """
  ALL
}

"""
The fields for querying Data Grid records.
"""
input DataGridInput {
  """
  Optionally specifies the Propeller to use. Applications may not set this value. Instead, Application Queries always use the Propeller configured on the Application.
  """
  propeller: Propeller
  """
  The ID of the Data Pool to be queried
  """
  dataPool: DataPoolInput
  """
  The time range for retrieving the records.
  """
  timeRange: TimeRangeInput!
  """
  The time zone to use. Dates and times are always returned in UTC, but setting the time zone influences relative time ranges and granularities.

  You can set this to "America/Los_Angeles", "Europe/Berlin", or any other value in the [IANA time zone database](https://en.wikipedia.org/wiki/Tz_database). Defaults to "UTC".
  """
  timeZone: String
  """
  The columns to retrieve.
  """
  columns: [String!]!
  """
  The index of the column to order the table by. The index is 1-based. If not provided, records will be ordered by their timestamp by default.
  """
  orderByColumn: Int
  """
  The sort order of the rows. It can be ascending (`ASC`) or descending (`DESC`) order. Defaults to descending (`DESC`) order when not provided.
  """
  sort: Sort
  """
  The filters to apply to the records. You may only filter on columns included in the `columns` array input.
  """
  filters: [FilterInput!]
  """
  The number of rows to be returned when paging forward. It can be a number between 1 and 1,000.
  """
  first: Int
  """
  The cursor to use when paging forward.
  """
  after: String
  """
  The number of rows to be returned when paging forward. It can be a number between 1 and 1,000.
  """
  last: Int
  """
  The cursor to use when paging backward.
  """
  before: String
}

"""
The fields for querying a Metric Report.

A Metric Report is a table whose columns include dimensions and Metric values, calculated over a given time range.
"""
input MetricReportInput {
  """
  Optionally specifies the Propeller to use. Applications may not set this value. Instead, Application Queries always use the Propeller configured on the Application.
  """
  propeller: Propeller
  """
  The time range for calculating the Metric Report.
  """
  timeRange: TimeRangeInput!
  """
  The time zone to use. Dates and times are always returned in UTC, but setting the time zone influences relative time ranges and granularities.

  You can set this to "America/Los_Angeles", "Europe/Berlin", or any other value in the [IANA time zone database](https://en.wikipedia.org/wiki/Tz_database). Defaults to "UTC".
  """
  timeZone: String
  """
  One or many dimensions to group the Metric values by. Typically, dimensions in a report are what you want to compare and rank.
  """
  dimensions: [MetricReportDimensionInput!]!
  """
  One or more Metrics to include in the Metric Report. These will be broken down by `dimensions`.
  """
  metrics: [MetricReportMetricInput!]!
  """
  The Query Filters to apply when building the Metric Report. These can be used to filter out rows.
  """
  filters: [FilterInput!]
  """
  The index of the column to order the Metric Report by. The index is 1-based and defaults to the first Metric column. In other words, by default, reports are ordered by the first Metric; however, you can order by the second Metric, third Metric, etc., by overriding the `orderByColumn` input. You can also order by dimensions this way.
  """
  orderByColumn: Int
  """
  The number of rows to be returned when paging forward. It can be a number between 1 and 1,000.
  """
  first: Int
  """
  The cursor to use when paging forward.
  """
  after: String
  """
  The number of rows to be returned when paging forward. It can be a number between 1 and 1,000.
  """
  last: Int
  """
  The cursor to use when paging backward.
  """
  before: String
}

"""
The fields for specifying a dimension to include in a Metric Report.
"""
input MetricReportDimensionInput {
  """
  The column name of the dimension to include in a Metric Report. This must match the name of a Data Pool column.
  """
  columnName: String!
  """
  The name to display in the `headers` array when displaying the report. This defaults to the column name if unspecified.
  """
  displayName: String
  """
  The sort order for the dimension. It can be ascending (`ASC`) or descending (`DESC`) order. Defaults to ascending (`ASC`) order when not provided.
  """
  sort: Sort
}

"""
The fields for specifying a Metric to include in a Metric Report.
"""
input MetricReportMetricInput {
  """
  The Metric's unique name. If not specified, Propel will lookup the Metric by ID.
  """
  uniqueName: String @deprecated(reason: "Use `metric`")
  """
  The Metric's ID. If not specified, Propel will lookup the Metric by unique name.
  """
  id: ID @deprecated(reason: "Use `metric`")
  """
  The Metric to query. You can query a pre-configured Metric by ID or name, or you can query an ad hoc Metric that you define inline.
  """
  metric: MetricInput
  """
  The name to display in the `headers` array when displaying the report. This defaults to the Metric's unique name if unspecified.
  """
  displayName: String
  """
  The Query Filters to apply when calculating the Metric.
  """
  filters: [FilterInput!]
  """
  The sort order for the Metric. It can be ascending (`ASC`) or descending (`DESC`) order. Defaults to descending (`DESC`) order when not provided.
  """
  sort: Sort
}

"""
The Metric Report connection object.

It includes `headers` and `rows` for a single page of a report. It also allows paging forward and backward to other
pages of the report.

Learn more about [pagination in GraphQL](https://www.propeldata.com/docs/api/pagination).
"""
type MetricReportConnection {
  """
  The report connection's page info.
  """
  pageInfo: PageInfo!
  """
  The report connection's edges.
  """
  edges: [MetricReportEdge!]!
  """
  The report connection's nodes.
  """
  nodes: [MetricReportNode!]!
  """
  An ordered array of display names for your dimensions and Metrics, as defined in the report input. Use this to display your table's header.
  """
  headers: [String]!
  """
  An ordered array of rows. Each row contains dimension and Metric values, as defined in the report input. Use these to display the rows of your table.
  """
  rows: [[String]!]!
  """
  The Query statistics and metadata.
  """
  query: QueryInfo!
}

"""
The Metric Report edge object.

Learn more about [pagination in GraphQL](https://www.propeldata.com/docs/api/pagination).
"""
type MetricReportEdge {
  """
  The edge's node.
  """
  node: MetricReportNode!
  """
  The edge's cursor.
  """
  cursor: String!
}

"""
The Metric Report node object.

This type represents a single row of a report.

Learn more about [pagination in GraphQL](https://www.propeldata.com/docs/api/pagination).
"""
type MetricReportNode {
  """
  An ordered array of display names for your dimensions and Metrics, as defined in the report input. Use this to display your table's header.
  """
  headers: [String]!
  """
  An ordered array of columns. Each column contains the dimension and Metric values for a single row, as defined in the report input. Use this to display a single row within your table.
  """
  row: [String]!
}

"""
The Metric object.

A Metric is a business indicator measured over time.

[Learn more about Metrics](/docs/key-concepts#metric).
"""
type Metric implements Node & Common {
  """
  The Metric's unique identifier.
  """
  id: ID!
  """
  The Metric's unique name.
  """
  uniqueName: String!
  """
  The Metric's description.
  """
  description: String!
  """
  The Metric's Account.
  """
  account: Account!
  """
  The Metric's Environment.
  """
  environment: Environment!
  """
  The Metric's creation date and time in UTC.
  """
  createdAt: DateTime!
  """
  The Metric's last modification date and time in UTC.
  """
  modifiedAt: DateTime!
  """
  The Metric's creator. It can be either a User ID, an Application ID, or "system" if it was created by Propel.
  """
  createdBy: String!
  """
  The Metric's last modifier. It can be either a User ID, an Application ID, or "system" if it was modified by Propel.
  """
  modifiedBy: String!
  """
  The Data Pool that powers this Metric.
  """
  dataPool: DataPool
  """
  The Metric's Dimensions. These Dimensions are available to Query Filters.
  """
  dimensions: [Dimension!]!
  """
  The Metric's timestamp. This is the same as its Data Pool's timestamp.
  """
  timestamp: Dimension!
  """
  The Metric's measure. Access this from the Metric's `settings` object instead.
  """
  measure: Dimension @deprecated(reason: "Use the Metric `settings` object instead.")
  """
  List the Boosters associated to the Metric.
  """
  boosters(first: Int, after: String, last: Int, before: String): BoosterConnection!
  """
  The Metric's type. The different Metric types determine how the values are calculated.
  """
  type: MetricType!
  """
  The settings for the Metric. The settings are specific to the Metric's type.
  """
  settings: MetricSettings!
  """
  The Metric data in counter format. A single metric value for the given time range and filters.
  """
  counter(input: CounterInput!): CounterResponse
  """
  The Metric data in time series format. Arrays of timestamps and Metric values for the given time range and filters.
  """
  timeSeries(input: TimeSeriesInput!): TimeSeriesResponse
  """
  The Metric data in leaderboard format. A table (array of rows) with the selected dimensions and corresponding Metric values for the given time range and filters.
  """
  leaderboard(input: LeaderboardInput!): LeaderboardResponse
  """
  List the Policies associated to the Metric.
  """
  policies(first: Int, after: String, last: Int, before: String): PolicyConnection!
    @deprecated(reason: "Use Data Pool Access Policies instead")
  """
  Whether or not access control is enabled for the Metric.
  """
  accessControlEnabled: Boolean! @deprecated(reason: "Use Data Pool Access Policies instead")
}

"""
A Metric's settings, depending on its type.
"""
union MetricSettings =
    CountMetricSettings
  | SumMetricSettings
  | CountDistinctMetricSettings
  | AverageMetricSettings
  | MinMetricSettings
  | MaxMetricSettings
  | CustomMetricSettings

"""
The available Metric types.
"""
enum MetricType {
  """
  Counts the number of records that matches the Metric Filters. For time series, it will count the values for each time granularity.
  """
  COUNT
  """
  Sums the values of the specified column for every record that matches the Metric Filters. For time series, it will sum the values for each time granularity.
  """
  SUM
  """
  Counts the number of distinct values in the specified column for every record that matches the Metric Filters. For time series, it will count the distinct values for each time granularity.
  """
  COUNT_DISTINCT
  """
  Averages the values of the specified column for every record that matches the Metric Filters. For time series, it will average the values for each time granularity.
  """
  AVERAGE
  """
  Selects the minimum value of the specified column for every record that matches the Metric Filters. For time series, it will select the minimum value for each time granularity.
  """
  MIN
  """
  Selects the maximum value of the specified column for every record that matches the Metric Filters. For time series, it will select the maximum value for each time granularity.
  """
  MAX
  "Aggregates values based on the provided custom expression."
  CUSTOM
}

"""
Settings for Count Metrics.
"""
type CountMetricSettings {
  """
  Metric Filters allow defining a Metric with a subset of records from the given Data Pool. If no Metric Filters are present, all records will be included. To filter at query time, add Dimensions and use the `filters` property on the `timeSeriesInput`, `counterInput`, or `leaderboardInput` objects. There is no need to add `filters` to be able to filter at query time.
  """
  filters: [Filter!]
}

"""
Settings for Sum Metrics.
"""
type SumMetricSettings {
  """
  Metric Filters allow defining a Metric with a subset of records from the given Data Pool. If no Metric Filters are present, all records will be included. To filter at query time, add Dimensions and use the `filters` property on the `timeSeriesInput`, `counterInput`, or `leaderboardInput` objects. There is no need to add `filters` to be able to filter at query time.
  """
  filters: [Filter!]
  """
  The Dimension to be summed.
  """
  measure: Dimension!
}

"""
Settings for Count Distinct Metrics.
"""
type CountDistinctMetricSettings {
  """
  Metric Filters allow defining a Metric with a subset of records from the given Data Pool. If no Metric Filters are present, all records will be included. To filter at query time, add Dimensions and use the `filters` property on the `timeSeriesInput`, `counterInput`, or `leaderboardInput` objects. There is no need to add `filters` to be able to filter at query time.
  """
  filters: [Filter!]
  """
  The Dimension where the count distinct operation is going to be performed.
  """
  dimension: Dimension!
}

"""
Settings for Average Metrics.
"""
type AverageMetricSettings {
  """
  Metric Filters allow defining a Metric with a subset of records from the given Data Pool. If no Metric Filters are present, all records will be included. To filter at query time, add Dimensions and use the `filters` property on the `timeSeriesInput`, `counterInput`, or `leaderboardInput` objects. There is no need to add `filters` to be able to filter at query time.
  """
  filters: [Filter!]
  """
  The Dimension to be averaged.
  """
  measure: Dimension!
}

"""
Settings for Min Metrics.
"""
type MinMetricSettings {
  """
  Metric Filters allow defining a Metric with a subset of records from the given Data Pool. If no Metric Filters are present, all records will be included. To filter at query time, add Dimensions and use the `filters` property on the `timeSeriesInput`, `counterInput`, or `leaderboardInput` objects. There is no need to add `filters` to be able to filter at query time.
  """
  filters: [Filter!]
  """
  The Dimension to select the minimum from.
  """
  measure: Dimension!
}

"""
Settings for Max Metrics.
"""
type MaxMetricSettings {
  """
  Metric Filters allow defining a Metric with a subset of records from the given Data Pool. If no Metric Filters are present, all records will be included. To filter at query time, add Dimensions and use the `filters` property on the `timeSeriesInput`, `counterInput`, or `leaderboardInput` objects. There is no need to add `filters` to be able to filter at query time.
  """
  filters: [Filter!]
  """
  The Dimension to select the maximum from.
  """
  measure: Dimension!
}

"""
Settings for Custom Metrics.
"""
type CustomMetricSettings {
  """
  Metric Filters allow defining a Metric with a subset of records from the given Data Pool. If no Metric Filters are present, all records will be included. To filter at query time, add Dimensions and use the `filters` property on the `timeSeriesInput`, `counterInput`, or `leaderboardInput` objects. There is no need to add `filters` to be able to filter at query time.
  """
  filters: [Filter!]
  """
  The expression that defines the aggregation function for this Metric.
  """
  expression: String!
}

"""
The result of a mutation which creates or modifies a Metric.
"""
type MetricResponse {
  """
  The Metric which was created or modified.
  """
  metric: Metric
}

"""
The fields for creating a new Count Metric.
"""
input CreateCountMetricInput {
  """
  The Data Pool that powers this Metric.
  """
  dataPool: ID!
  """
  The Metric's unique name. If not specified, Propel will set the ID as unique name.
  """
  uniqueName: String
  """
  The Metric's description.
  """
  description: String
  """
  The Metric's Filters. Metric Filters allow defining a Metric with a subset of records from the given Data Pool. If no Filters are present, all records will be included.
  """
  filters: [FilterInput!]
  """
  The Metric's Dimensions. Dimensions define the columns that will be available to filter the Metric at query time.
  """
  dimensions: [DimensionInput!]
}

"""
The fields for creating a new Sum Metric.
"""
input CreateSumMetricInput {
  """
  The Data Pool that powers this Metric.
  """
  dataPool: ID!
  """
  The Metric's unique name. If not specified, Propel will set the ID as unique name.
  """
  uniqueName: String
  """
  The Metric's description.
  """
  description: String
  """
  The Metric's Filters. Metric Filters allow defining a Metric with a subset of records from the given Data Pool. If no Filters are present, all records will be included.
  """
  filters: [FilterInput!]
  """
  The Metric's Dimensions. Dimensions define the columns that will be available to filter the Metric at query time.
  """
  dimensions: [DimensionInput!]
  """
  The column to be summed.
  """
  measure: DimensionInput!
}

"""
The fields for creating a new Average Metric.
"""
input CreateAverageMetricInput {
  """
  The Data Pool that powers this Metric.
  """
  dataPool: ID!
  """
  The Metric's unique name.
  """
  uniqueName: String
  """
  The Metric's description.
  """
  description: String
  """
  The Metric's Filters. Metric Filters allow defining a Metric with a subset of records from the given Data Pool. If no Filters are present, all records will be included.
  """
  filters: [FilterInput!]
  """
  The Metric's Dimensions. Dimensions define the columns that will be available to filter the Metric at query time.
  """
  dimensions: [DimensionInput!]
  """
  The column to be averaged.
  """
  measure: DimensionInput!
}

"""
The fields for creating a new Minimum (Min) Metric.
"""
input CreateMinMetricInput {
  """
  The Data Pool that powers this Metric.
  """
  dataPool: ID!
  """
  The Metric's unique name. If not specified, Propel will set the ID as unique name.
  """
  uniqueName: String
  """
  The Metric's description.
  """
  description: String
  """
  The Metric's Filters. Metric Filters allow defining a Metric with a subset of records from the given Data Pool. If no Filters are present, all records will be included.
  """
  filters: [FilterInput!]
  """
  The Metric's Dimensions. Dimensions define the columns that will be available to filter the Metric at query time.
  """
  dimensions: [DimensionInput!]
  measure: DimensionInput!
}

"""
The fields for creating a new Maximum (Max) Metric.
"""
input CreateMaxMetricInput {
  """
  The Data Pool that powers this Metric.
  """
  dataPool: ID!
  """
  The Metric's unique name. If not specified, Propel will set the ID as unique name.
  """
  uniqueName: String
  """
  The Metric's description.
  """
  description: String
  """
  The Metric's Filters. Metric Filters allow defining a Metric with a subset of records from the given Data Pool. If no Filters are present, all records will be included.
  """
  filters: [FilterInput!]
  """
  The Metric's Dimensions. Dimensions define the columns that will be available to filter the Metric at query time.
  """
  dimensions: [DimensionInput!]
  """
  The column to calculate the maximum from.
  """
  measure: DimensionInput!
}

"""
The fields for creating a new Count Distinct Metric.
"""
input CreateCountDistinctMetricInput {
  """
  The Data Pool that powers this Metric.
  """
  dataPool: ID!
  """
  The Metric's unique name. If not specified, Propel will set the ID as unique name.
  """
  uniqueName: String
  """
  The Metric's description.
  """
  description: String
  """
  The Metric's Filters. Metric Filters allow defining a Metric with a subset of records from the given Data Pool. If no Filters are present, all records will be included.
  """
  filters: [FilterInput!]
  """
  The Metric's Dimensions. Dimensions define the columns that will be available to filter the Metric at query time.
  """
  dimensions: [DimensionInput!]
  """
  The Dimension over which the count distinct operation is going to be performed.
  """
  dimension: DimensionInput!
}

"""
The fields for creating a new Custom Metric.
"""
input CreateCustomMetricInput {
  """
  The Data Pool that powers this Metric.
  """
  dataPool: ID!
  """
  The Metric's unique name. If not specified, Propel will set the ID as unique name.
  """
  uniqueName: String
  """
  The Metric's description.
  """
  description: String
  """
  The Metric's Filters. Metric Filters allow defining a Metric with a subset of records from the given Data Pool. If no Filters are present, all records will be included.
  """
  filters: [FilterInput!]
  """
  The Metric's Dimensions. Dimensions define the columns that will be available to filter the Metric at query time.
  """
  dimensions: [DimensionInput!]
  """
  The expression that defines the aggregation function for this Metric.
  """
  expression: String!
}

"""
Response returned by the validateExpression query for validating expressions in Custom Metrics.

Returns whether the expression is valid or not with a reason explaining why.
"""
type ValidateExpressionResult {
  " True if the expression is valid, false otherwise. "
  valid: Boolean!
  " The reason for why the expression is not valid in case it isn't, null otherwise.  "
  reason: String
}

"""
The fields for modifying a Metric.
"""
input ModifyMetricInput {
  """
  The ID of the Metric to modify.
  """
  metric: ID!
  """
  The Metric's new unique name.
  """
  uniqueName: String
  """
  The Metric's new description.
  """
  description: String
  """
  The Metric's new Dimensions. Used to add or remove Dimensions.
  """
  dimensions: [DimensionInput!]
  """
  The Metric's new Filters. Used to add or remove Metric Filters.
  """
  filters: [FilterInput!]
  """
  Enables or disables access control for the Metric.
  """
  accessControlEnabled: Boolean
}

"""
The fields for migrating a Metric's Data Pool.
"""
input MigrateMetricInput {
  """
  The Metric that is going to be migrated.
  """
  metricId: ID!
  """
  The DataPool to which the Metric is going to be migrated.
  """
  newDataPoolId: ID!
}

"""
The fields for querying a Data Pool.
"""
input DataPoolInput @oneOf {
  "The ID of the Data Pool."
  id: ID
  "The name of the Data Pool."
  name: String
}

"""
The fields for querying a Custom Metric.
"""
input CustomMetricQueryInput {
  "The Data Pool to which this Metric belongs."
  dataPool: DataPoolInput!
  "Custom expression for defining the Metric."
  expression: String!
}

"""
The fields for querying a Count Metric.
"""
input CountMetricQueryInput {
  "The Data Pool to which this Metric belongs."
  dataPool: DataPoolInput!
}

"""
The fields for querying a Sum Metric.
"""
input SumMetricQueryInput {
  "The Data Pool to which this Metric belongs."
  dataPool: DataPoolInput!
  """
  The column to be summed.
  """
  measure: DimensionInput!
}

"""
The fields for querying an Average Metric.
"""
input AverageMetricQueryInput {
  "The Data Pool to which this Metric belongs."
  dataPool: DataPoolInput!
  """
  The column to be averaged.
  """
  measure: DimensionInput!
}

"""
The fields for querying a Min Metric.
"""
input MinMetricQueryInput {
  "The Data Pool to which this Metric belongs."
  dataPool: DataPoolInput!
  """
  The column to calculate the minimum from.
  """
  measure: DimensionInput!
}

"""
The fields for querying a Max Metric.
"""
input MaxMetricQueryInput {
  "The Data Pool to which this Metric belongs."
  dataPool: DataPoolInput!
  """
  The column to calculate the maximum from.
  """
  measure: DimensionInput!
}

"""
The fields for querying a Count Distinct Metric.
"""
input CountDistinctMetricQueryInput {
  "The Data Pool to which this Metric belongs."
  dataPool: DataPoolInput!
  """
  The column to count distinct values from.
  """
  dimension: DimensionInput!
}

"""
The fields for querying a Metric.
The metric query is used to query a Metric in counter, time series, or leaderboard format.
"""
input MetricInput @oneOf {
  "The ID of a pre-configured Metric."
  id: ID
  "The name of a pre-configured Metric."
  name: String
  "An ad hoc Custom Metric."
  custom: CustomMetricQueryInput
  "An ad hoc Count Metric."
  count: CountMetricQueryInput
  "An ad hoc Sum Metric."
  sum: SumMetricQueryInput
  "An ad hoc Average Metric."
  average: AverageMetricQueryInput
  "An ad hoc Min Metric."
  min: MinMetricQueryInput
  "An ad hoc Max Metric."
  max: MaxMetricQueryInput
  "An ad hoc Count Distinct Metric."
  countDistinct: CountDistinctMetricQueryInput
}

"""
The fields for querying a Metric in counter format.

A Metric's counter query returns a single value over a given time range.
"""
input CounterInput {
  """
  The ID of the Metric to query.

  Required if `metricName` is not specified.
  """
  metricId: ID @deprecated(reason: "Use `metric`")
  """
  The name of the Metric to query.

  Required if `metricId` is not specified.
  """
  metricName: String @deprecated(reason: "Use `metric`")
  """
  The Metric to query. You can query a pre-configured Metric by ID or name, or you can query an ad hoc Metric that you define inline.
  """
  metric: MetricInput
  """
  The time range for calculating the counter.
  """
  timeRange: TimeRangeInput!
  """
  The time zone to use. Dates and times are always returned in UTC, but setting the time zone influences relative time ranges and granularities.

  You can set this to "America/Los_Angeles", "Europe/Berlin", or any other value in the [IANA time zone database](https://en.wikipedia.org/wiki/Tz_database). Defaults to "UTC".
  """
  timeZone: String
  """
  The Query Filters to apply before retrieving the counter data. If no Query Filters are provided, all data is included.
  """
  filters: [FilterInput!]
  """
  Optionally specifies the Propeller to use. This can be set when querying from the Metric Playground or GraphQL Explorer. Applications may not set this value. Instead, Application Queries always use the Propeller configured on the Application.
  """
  propeller: Propeller
}

"""
The fields for querying a Metric in time series format.

A Metric's time series query returns the values over a given time range aggregated by a given time granularity; day, month, or year, for example.
"""
input TimeSeriesInput {
  """
  The ID of the Metric to query.

  Required if `metricName` is not specified.
  """
  metricId: ID @deprecated(reason: "Use `metric`")
  """
  The name of the Metric to query.

  Required if `metricId` is not specified.
  """
  metricName: String @deprecated(reason: "Use `metric`")
  """
  The Metric to query. You can query a pre-configured Metric by ID or name, or you can query an ad hoc Metric that you define inline.
  """
  metric: MetricInput
  """
  The time range for calculating the time series.
  """
  timeRange: TimeRangeInput!
  """
  The time zone to use. Dates and times are always returned in UTC, but setting the time zone influences relative time ranges and granularities.

  You can set this to "America/Los_Angeles", "Europe/Berlin", or any other value in the [IANA time zone database](https://en.wikipedia.org/wiki/Tz_database). Defaults to "UTC".
  """
  timeZone: String
  """
  The time granularity (hour, day, month, etc.) to aggregate the Metric values by.
  """
  granularity: TimeSeriesGranularity!
  """
  The Query Filters to apply before retrieving the time series data. If no Query Filters are provided, all data is included.
  """
  filters: [FilterInput!]
  """
  Optionally specifies the Propeller to use. This can be set by Users when querying from the Metric Playground or GraphQL Explorer. Applications may not set this value. Instead, Application Queries always use the Propeller configured on the Application.
  """
  propeller: Propeller
}

"""
The fields for querying a Metric in leaderboard format.

A Metric's leaderboard query returns an ordered table of Dimension and Metric values over a given time range.
"""
input LeaderboardInput {
  """
  The ID of the Metric to query.

  Required if `metricName` is not specified.
  """
  metricId: ID @deprecated(reason: "Use `metric`")
  """
  The name of the Metric to query.

  Required if `metricId` is not specified.
  """
  metricName: String @deprecated(reason: "Use `metric`")
  """
  The Metric to query. You can query a pre-configured Metric by ID or name, or you can query an ad hoc Metric that you define inline.
  """
  metric: MetricInput
  """
  The time range for calculating the leaderboard.
  """
  timeRange: TimeRangeInput!
  """
  The time zone to use. Dates and times are always returned in UTC, but setting the time zone influences relative time ranges and granularities.

  You can set this to "America/Los_Angeles", "Europe/Berlin", or any other value in the [IANA time zone database](https://en.wikipedia.org/wiki/Tz_database). Defaults to "UTC".
  """
  timeZone: String
  """
  One or many Dimensions to group the Metric values by. Typically, Dimensions in a leaderboard are what you want to compare and rank.
  """
  dimensions: [DimensionInput!]!
  """
  The sort order of the rows. It can be ascending (`ASC`) or descending (`DESC`) order. Defaults to descending (`DESC`) order when not provided.
  """
  sort: Sort
  """
  The number of rows to be returned. It can be a number between 1 and 1,000.
  """
  rowLimit: Int!
  """
  The list of filters to apply before retrieving the leaderboard data. If no Query Filters are provided, all data is included.
  """
  filters: [FilterInput!]
  """
  Optionally specifies the Propeller to use. This can be set by Users when querying from the Metric Playground or GraphQL Explorer. Applications may not set this value. Instead, Application Queries always use the Propeller configured on the Application.
  """
  propeller: Propeller
}

"""
The fields required to specify the time range for a time series, counter, or leaderboard Metric query.

If no relative or absolute time ranges are provided, Propel defaults to an absolute time range beginning with the earliest record in the Metric's Data Pool and ending with the latest record.

If both relative and absolute time ranges are provided, the relative time range will take precedence.

If a `LAST_N` relative time period is selected, an `n` ≥ 1 must be provided. If no `n` is provided or `n` < 1, a `BAD_REQUEST` error will be returned.
"""
input TimeRangeInput {
  """
  The relative time period.
  """
  relative: RelativeTimeRange
  """
  The number of time units for the `LAST_N` relative periods.
  """
  n: Int
  """
  The optional start timestamp (inclusive). Defaults to the timestamp of the earliest record in the Data Pool.
  """
  start: DateTime
  """
  The optional end timestamp (exclusive). Defaults to the timestamp of the latest record in the Data Pool.
  """
  stop: DateTime
}

"""
The available sort orders.
"""
enum Sort {
  """
  Sort in ascending order.
  """
  ASC
  """
  Sort in descending order.
  """
  DESC
}

"""
The Relative time ranges are based on the current date and time.

`THIS` - The current unit of time. For example, if today is June 8, 2022, and
`THIS_MONTH` is selected, then data for June 2022 would be returned.

`PREVIOUS` - The previous unit of time. For example, if today is June 8, 2022, and
`PREVIOUS_MONTH` is selected, then data for May 2022 would be returned. It excludes
the current unit of time.

`NEXT` - The next unit of time. For example, if today is June 8, 2022, and
`NEXT_MONTH` is selected, then data for July 2022 would be returned. It excludes
the current unit of time.

`LAST_N` - The last `n` units of time, including the current one. For example, if today
is June 8, 2022 and `LAST_N_YEARS` with `n` = 3 is selected, then data for 2020, 2021, and
2022 will be returned. It will include the current time period.
"""
enum RelativeTimeRange {
  """
  Starts at the zeroth minute of the current hour and continues for 60 minutes.
  """
  THIS_HOUR
  """
  Starts at 12:00:00 AM of the current day and continues for 24 hours.
  """
  TODAY
  """
  Starts on Monday, 12:00:00 AM of the current week and continues for seven days.
  """
  THIS_WEEK
  """
  Starts at 12:00:00 AM on the first day of the current month and continues for the duration of the month.
  """
  THIS_MONTH
  """
  Starts at 12:00:00 AM on the first day of the current calendar quarter and continues for the duration of the quarter.
  """
  THIS_QUARTER
  """
  Starts on January 1st, 12:00:00 AM of the current year and continues for the duration of the year.
  """
  THIS_YEAR
  """
  Starts at the zeroth minute of the previous hour and continues for 60 minutes.
  """
  PREVIOUS_HOUR
  """
  Starts at 12:00:00 AM on the day before the today and continues for 24 hours.
  """
  YESTERDAY
  """
  Starts on Monday, 12:00:00 AM, a week before the current week, and continues for seven days.
  """
  PREVIOUS_WEEK
  """
  Starts at 12:00:00 AM on the first day of the month before the current month and continues for the duration of the month.
  """
  PREVIOUS_MONTH
  """
  Starts at 12:00:00 AM on the first day of the calendar quarter before the current quarter and continues for the duration of the quarter.
  """
  PREVIOUS_QUARTER
  """
  Starts on January 1st, 12:00:00 AM, the year before the current year, and continues for the duration of the year.
  """
  PREVIOUS_YEAR
  """
  Starts at the zeroth minute of the next hour and continues for 60 minutes.
  """
  NEXT_HOUR
  """
  " Starts at 12:00:00 AM, the day after the current day, and continues for 24 hours.
  """
  TOMORROW
  """
  Starts on Monday, 12:00:00 AM, the week after the current week, and continues for the duration of the week.
  """
  NEXT_WEEK
  """
  Starts at 12:00:00 AM on the first day of the next month and continues for the duration of the month.
  """
  NEXT_MONTH
  """
  Starts at 12:00:00 AM on the first day of the next calendar quarter and continues for the duration of the quarter.
  """
  NEXT_QUARTER
  """
  Starts on January 1st, 12:00:00 AM of the next year and continues for the duration of the year.
  """
  NEXT_YEAR
  """
  Starts at the zeroth second `n` - 1 minute(s) before the current minute and continues through the current minute. It includes this minute.
  """
  LAST_N_MINUTES
  """
  Starts at the zeroth minute of the `n` - 1 hour(s) before the current hour, and continues through the current hour. It includes this hour.
  """
  LAST_N_HOURS
  """
  Starts at 12:00:00 AM, `n` - 1 day(s) before the current day, and continues through the current day. It includes today.
  """
  LAST_N_DAYS
  """
  Starts on Monday, 12:00:00 AM, `n` - 1 week(s) before the current week, and continues through the current week. It includes this week.
  """
  LAST_N_WEEKS
  """
  Starts at 12:00:00 AM on the first day of the month, `n` - 1 month(s) before the current month, and continues through the current month. It includes this month.
  """
  LAST_N_MONTHS
  """
  Starts at 12:00:00 AM on the first day of the calendar quarter `n` - 1 quarter(s) before the current quarter and continues through the current quarter. It includes this quarter.
  """
  LAST_N_QUARTERS
  """
  Starts on January 1st, 12:00:00 AM of the year `n` - 1 year(s) before the current year and continues through the current year. It includes this year.
  """
  LAST_N_YEARS
  LAST_15_MINUTES @deprecated(reason: "Use `LAST_N_MINUTES` instead.")
  LAST_30_MINUTES @deprecated(reason: "Use `LAST_N_MINUTES` instead.")
  LAST_HOUR @deprecated(reason: "Use `LAST_N_HOURS` instead.")
  LAST_4_HOURS @deprecated(reason: "Use `LAST_N_HOURS` instead.")
  LAST_12_HOURS @deprecated(reason: "Use `LAST_N_HOURS` instead.")
  LAST_24_HOURS @deprecated(reason: "Use `LAST_N_HOURS` instead.")
  LAST_7_DAYS @deprecated(reason: "Use `LAST_N_DAYS` instead.")
  LAST_30_DAYS @deprecated(reason: "Use `LAST_N_DAYS` instead.")
  LAST_90_DAYS @deprecated(reason: "Use `LAST_N_DAYS` instead.")
  LAST_3_MONTHS @deprecated(reason: "Use `LAST_N_MONTHS` instead.")
  LAST_6_MONTHS @deprecated(reason: "Use `LAST_N_MONTHS` instead.")
  LAST_YEAR @deprecated(reason: "Use `LAST_N_YEARS` instead.")
  LAST_2_YEARS @deprecated(reason: "Use `LAST_N_YEARS` instead.")
  LAST_5_YEARS @deprecated(reason: "Use `LAST_N_YEARS` instead.")
}

"""
The available time series granularities. Granularities define the unit of time to aggregate the Metric data for a time series query.

For example, if the granularity is set to `DAY`, then the the time series query will return a label and a value for each day.

If there are no records for a given time series granularity, Propel will return the label and a value of "0" so that the time series can be properly visualized.
"""
enum TimeSeriesGranularity {
  """
  Aggregates values by minute intervals.
  """
  MINUTE
  """
  Aggregates values by 5-minute intervals.
  """
  FIVE_MINUTES
  """
  Aggregates values by 10-minute intervals.
  """
  TEN_MINUTES
  """
  Aggregates values by 15-minute intervals.
  """
  FIFTEEN_MINUTES
  """
  Aggregates values by hourly intervals.
  """
  HOUR
  """
  Aggregates values by daily intervals.
  """
  DAY
  """
  Aggregates values by weekly intervals.
  """
  WEEK
  """
  Aggregates values by monthly intervals.
  """
  MONTH
  """
  Aggregates values by yearly intervals.
  """
  YEAR
}

"""
The fields of a filter.

You can construct more complex filters using `and` and `or`. For example, to construct a filter equivalent to

```
(value > 0 AND value <= 100) OR status = "confirmed"
```

you could write

```
{
  "column": "value",
  "operator": "GREATER_THAN",
  "value": "0",
  "and": [{
    "column": "value",
    "operator": "LESS_THAN_OR_EQUAL_TO",
    "value": "0"
  }],
  "or": [{
    "column": "status",
    "operator": "EQUALS",
    "value": "confirmed"
  }]
}
```

Note that `and` takes precedence over `or`.
"""
input FilterInput {
  """
  The name of the column to filter on.
  """
  column: String!
  """
  The operation to perform when comparing the column and filter values.
  """
  operator: FilterOperator!
  """
  The value to compare the column to.
  """
  value: String
  """
  Additional filters to AND with this one. AND takes precedence over OR.
  """
  and: [FilterInput!]
  """
  Additional filters to OR with this one. AND takes precedence over OR.
  """
  or: [FilterInput!]
}

"""
The available Filter operators.
"""
enum FilterOperator {
  """
  Selects values that are equal to the specified value.
  """
  EQUALS
  """
  Selects values that are not equal to the specified value.
  """
  NOT_EQUALS
  """
  Selects values that are greater than the specified value.
  """
  GREATER_THAN
  """
  Selects values that are greater or equal to the specified value.
  """
  GREATER_THAN_OR_EQUAL_TO
  """
  Selects values that are less than the specified value.
  """
  LESS_THAN
  """
  Selects values that are less or equal to the specified value.
  """
  LESS_THAN_OR_EQUAL_TO
  """
  Selects values that are null. This operator does not accept a value.
  """
  IS_NULL
  """
  Selects values that are not null. This operator does not accept a value.
  """
  IS_NOT_NULL
  """
  Selects values that match the specified pattern.
  """
  LIKE
  """
  "Selects values that do not match the specified pattern.
  """
  NOT_LIKE
}

"""
The fields for creating an Update Data Pool Records Job.

```
{
"column": "status",
"expression": "'completed'"
}

{
"column": "counter",
"expression": "counter + 1"
}

{
"column": "full_name",
"expression": "concat(first_name, ' ', last_name)"
}
```
"""
input UpdateDataPoolRecordsJobSetColumnInput {
  """
  The name of the column to update.
  """
  column: String!
  """
  The value to which the column will be updated. Once evaluated, it should be of the same data type as the column.
  """
  expression: String!
}

"""
The time series response object. It contains an array of time series labels and an array of Metric values for the given time range and Query Filters.
"""
type TimeSeriesResponse {
  """
  The time series labels.
  """
  labels: [String!]!
  """
  The time series values.
  """
  values: [String]!
  """
  The Query statistics and metadata.
  """
  query: QueryInfo!
}

"""
The counter response object. It contains a single Metric value for the given time range and Query Filters.
"""
type CounterResponse {
  """
  The value of the counter.
  """
  value: String
  """
  The Query statistics and metadata.
  """
  query: QueryInfo!
}

"""
The leaderboard response object. It contains an array of headers and a table (array of rows) with the selected Dimensions and corresponding Metric values for the given time range and Query Filters.
"""
type LeaderboardResponse {
  """
  The table headers. It contains the Dimension and Metric names.
  """
  headers: [String!]!
  """
  An ordered array of rows. Each row contains the Dimension values and the corresponding Metric value. A Dimension value can be empty. A Metric value will never be empty.
  """
  rows: [[String]!]!
  """
  The Query statistics and metadata.
  """
  query: QueryInfo!
}

"""
The Query Info object. It contains metadata and statistics about a Query performed.
"""
type QueryInfo {
  """
  The Query's unique identifier.
  """
  id: ID!
  """
  The date and time in UTC when the Query was created.
  """
  createdAt: DateTime!
  """
  The unique identifier of the actor that performed the Query.
  """
  createdBy: String!
  """
  The date and time in UTC when the Query was last modified.
  """
  modifiedAt: DateTime!
  """
  The unique identifier of the actor that modified the Query.
  """
  modifiedBy: String!
  """
  The bytes processed by the Query.
  """
  bytesProcessed: String!
  """
  The duration of the Query in milliseconds.
  """
  durationInMilliseconds: Int!
  """
  The number of records processed by the Query.
  """
  recordsProcessed: String!
  """
  The bytes returned by the Query.
  """
  resultingBytes: Int!
  """
  The number of records returned by the Query.
  """
  resultingRecords: Int!
  """
  If the Query was boosted, the Booster that was used.
  """
  booster: Booster
  """
  The Propeller used for this query.
  """
  propeller: Propeller
  """
  The Query status.
  """
  status: QueryStatus!
  """
  The Query type.
  """
  type: QueryType!
  """
  The Query subtype.
  """
  subtype: QuerySubtype
}

"""
The Query status.
"""
enum QueryStatus {
  """
  The Query was completed succesfully.
  """
  COMPLETED
  """
  The Query experienced an error.
  """
  ERROR
  """
  The Query timed out.
  """
  TIMED_OUT
}

"""
The Query type.
"""
enum QueryType {
  """
  Indicates a Metric Query.
  """
  METRIC
  """
  Indicates a Dimension Stats Query.
  """
  STATS
  """
  Indicates a Report Query.
  """
  REPORT
  """
  Indicates a Record Table Query.
  """
  RECORDS
  """
  Indicates records queried by unique ID.
  """
  RECORDS_BY_UNIQUE_ID
  """
  Indicates a SelectV1 Query.
  """
  SELECT
  """
  Indicates a SQL Query.
  """
  SQL
  """
  Indicates a Top Values Query.
  """
  TOP_VALUES
}

"""
The Query subtype.
"""
enum QuerySubtype {
  """
  Indicates a Metric counter Query.
  """
  COUNTER
  """
  Indicates a Metric time series Query.
  """
  TIME_SERIES
  """
  Indicates a Metric leaderboard Query.
  """
  LEADERBOARD
}

"""
The Booster status.
"""
enum BoosterStatus {
  """
  The Booster has been created. Propel will start optimizing the Data Pool soon.
  """
  CREATED
  """
  Propel is setting up the Booster and optimizing the Data Pool.
  """
  OPTIMIZING
  """
  The Booster is now live and available to speed up Metric queries.
  """
  LIVE
  """
  Propel failed to setup the Booster. Please write to support. Alternatively, you can delete the Booster and try again.
  """
  FAILED
  """
  Propel is deleting the Booster and all of its associated data.
  """
  DELETING
}

"""
Boosters allow you to optimize Metric Queries for a subset of commonly used Dimensions. A Metric can have one or many Boosters to optimize for the different Query patterns.

Boosters can be understood as an aggregating index. The index is formed from left to right as follows:

1. The Data Pool's Tenant ID column (if present)
2. Metric Filter columns (if present)
3. Query Filter Dimensions (see `dimensions`)
4. The Data Pool's timestamp column
"""
type Booster implements Node {
  """
  The Booster's unique identifier.
  """
  id: ID!
  """
  The Booster's Account.
  """
  account: Account!
  """
  The Booster's Environment.
  """
  environment: Environment!
  """
  The Booster's creation date and time in UTC.
  """
  createdAt: DateTime!
  """
  The Booster's last modification date and time in UTC.
  """
  modifiedAt: DateTime!
  """
  The Booster's creator. It can be either a User ID, an Application ID, or "system" if it was created by Propel.
  """
  createdBy: String!
  """
  The Booster's last modifier. It can be either a User ID, an Application ID, or "system" if it was modified by Propel.
  """
  modifiedBy: String!
  """
  The Metric this Booster is associated to.
  """
  metric: Metric!
  """
  The status of the Booster (once LIVE it will be available for speeding up Metric queries).
  """
  status: BoosterStatus!
  """
  If the Booster fails during the optimization process, this field includes a descriptive
  error message.
  """
  error: Error
  """
  When the Booster is OPTIMIZING, this represents its progress as a number from 0 to 1.
  In all other states, progress is null.
  """
  progress: Float
  """
  Dimensions included in the Booster.
  """
  dimensions: [Dimension!]!
  """
  The number of records in the Booster.
  """
  recordCount: String
  """
  The amount of storage in terabytes used by the Booster.
  """
  sizeInTerabytes: Float
}

"""
The fields for creating a new Booster.

Boosters can be understood as an aggregating index. The index is formed from left to right as follows:

1. The Data Pool's Tenant ID column (if present)
2. Metric Filter columns (if present)
3. Query Filter Dimensions (see `dimensions`)
4. The Data Pool's timestamp column
"""
input CreateBoosterInput {
  """
  The Booster's Metric.
  """
  metric: ID!
  """
  Dimensions to include in the Booster.

  Follow these guidelines when specifying Dimensions:

  1. Specify Dimensions in descending order of importance for filtering and in ascending order of cardinality.
  2. Take into consideration hierarchical relationships as well (for example, a "country" Dimension should appear before a "state" Dimension).
  """
  dimensions: [DimensionInput!]!
}

"""
The result of a mutation which creates or modifies a Booster.
"""
type BoosterResponse {
  """
  The Booster which was created or modified.
  """
  booster: Booster
}

"""
The types of Policies that can be applied to a Metric.
"""
enum PolicyType {
  """
  Grants access to all Metric data.
  """
  ALL_ACCESS
  """
  Grants access to a specified tenant's Metric data.
  """
  TENANT_ACCESS
}

"""
The Policy type. It governs an Application's access to a Metric's data.
"""
type Policy implements Node {
  """
  The Policy's unique identifier.
  """
  id: ID!
  """
  The Policy's Account.
  """
  account: Account!
  """
  The Policy's Environment.
  """
  environment: Environment!
  """
  The Policy's creation date and time in UTC.
  """
  createdAt: DateTime!
  """
  The Policy's last modification date and time in UTC.
  """
  modifiedAt: DateTime!
  """
  The Policy's creator. It can be either a User ID, an Application ID, or "system" if it was created by Propel.
  """
  createdBy: String!
  """
  The Policy's last modifier. It can be either a User ID, an Application ID, or "system" if it was modified by Propel.
  """
  modifiedBy: String!
  """
  The type of Policy.
  """
  type: PolicyType!
  """
  The Application that is granted access.
  """
  application: Application!
  """
  The Metric that the Application is granted access to.
  """
  metric: Metric!
}

"""
The fields for creating a Policy.
"""
input CreatePolicyInput {
  """
  The Metric to which the Policy will be applied.
  """
  metric: ID!
  """
  The type of Policy to create.
  """
  type: PolicyType!
  """
  The Application that will be granted access to the Metric.
  """
  application: ID!
}

"""
The fields for modifying a Policy.
"""
input ModifyPolicyInput {
  """
  The Policy's unique identifier.
  """
  policy: ID!
  """
  The type of Policy.
  """
  type: PolicyType!
}

"""
The result of a mutation which creates or modifies a Policy.
"""
type PolicyResponse {
  """
  The Policy which was created or modified.
  """
  policy: Policy
}

"""
A Job is an asynchronous process that modifies a Data Pool or its records. It keeps track of its progress and outcome.
"""
interface Job {
  "The Job's creation date and time in UTC."
  createdAt: DateTime!
  "Who created the Job."
  createdBy: String!
  "The Job's last modification date and time in UTC."
  modifiedAt: DateTime!
  "Who last modified the Job."
  modifiedBy: String!
  "Account to which the Job belongs."
  account: Account!
  "Environment to which the Job belongs."
  environment: Environment!
  "The Data Pool whose records will be modified by the Job."
  dataPool: DataPool!
  "The current Job's status."
  status: JobStatus!
  "The error that occurred while executing the Job, if any."
  error: Error
  "The current progress of the Job, from 0.0 to 1.0."
  progress: Float!
  "The time at which the Job started."
  startedAt: DateTime
  "The time at which the Job succeeded."
  succeededAt: DateTime
  "The time at which the Job failed."
  failedAt: DateTime
}

enum JobStatus {
  "The Job was created, but is not yet being executed."
  CREATED
  "The Job is executing."
  IN_PROGRESS
  "The Job succeeded."
  SUCCEEDED
  "The Job failed. Check the error message."
  FAILED
}

"""
Deletion Job scheduled for a specific Data Pool.

The Deletion Job represents the asynchronous process of deleting data
given some filters inside a Data Pool. It tracks the deletion process
until it is finished, showing the progress and the outcome when it is finished.
"""
type DeletionJob implements Node & Job {
  "The Deletion Job's ID."
  id: ID!
  "The Deletion Job's creation date and time in UTC."
  createdAt: DateTime!
  "Who created the Deletion Job."
  createdBy: String!
  "The Deletion Job's last modification date and time in UTC."
  modifiedAt: DateTime!
  "Who last modified the Deletion Job."
  modifiedBy: String!
  "Account to which the Deletion Job belongs."
  account: Account!
  "Environment to which the Deletion Job belongs."
  environment: Environment!
  "The Data Pool whose records will be deleted by the Deletion Job."
  dataPool: DataPool!
  "The current Deletion Job's status."
  status: JobStatus!
  "The list of filters that will be used for deleting data. Data matching the filters will be deleted."
  filters: [Filter!]!
  "The error that occurred while deleting data, if any."
  error: Error
  "The current progress of the Deletion Job, from 0.0 to 1.0."
  progress: Float!
  "The time at which the Deletion Job started."
  startedAt: DateTime
  "The time at which the Deletion Job succeeded."
  succeededAt: DateTime
  "The time at which the Deletion Job failed."
  failedAt: DateTime
}

"""
The response returned by the Deletion Job.
"""
type RequestDeleteResponse {
  "The Deletion Job that was just created."
  job: DeletionJob!
}

"""
The response returned by the Deletion Job.
"""
type DeletionJobResponse {
  "The Deletion Job that was just created."
  job: DeletionJob!
}

"""
The fields for creating a Deletion Job.
"""
input DeletionRequestInput {
  "The Data Pool that is going to get the data deleted"
  dataPool: ID!
  "The list of filters that will be used for deleting data. Data matching these filters will be deleted."
  filters: [FilterInput!]!
}

"""
The fields for creating a Deletion Job.
"""
input CreateDeletionJobInput {
  "The Data Pool that is going to get the data deleted"
  dataPool: ID!
  "The list of filters that will be used for deleting data. Data matching these filters will be deleted."
  filters: [FilterInput!]!
}

"""
AddColumnToDataPoolJob scheduled for a specific Data Pool.

The Add Column Job represents the asynchronous process of adding a column,
given its name and type, to a Data Pool. It tracks the process of adding a column
until it is finished, showing the progress and the outcome when it is finished.
"""
type AddColumnToDataPoolJob implements Node & Job {
  "The AddColumnToDataPoolJob's ID."
  id: ID!
  "The AddColumnToDataPoolJob's creation date and time in UTC."
  createdAt: DateTime!
  "Who created the AddColumnToDataPoolJob."
  createdBy: String!
  "The AddColumnToDataPoolJob's last modification date and time in UTC."
  modifiedAt: DateTime!
  "Who modified the AddColumnToDataPoolJob last."
  modifiedBy: String!
  "Account to which the AddColumnToDataPoolJob belongs."
  account: Account!
  "Environment to which the AddColumnToDataPoolJob belongs."
  environment: Environment!
  "The Data Pool to which a column will be added by the Job."
  dataPool: DataPool!
  "The current AddColumnToDataPoolJob's status."
  status: JobStatus!
  "Name of the new column."
  columnName: String!
  "Type of the new column."
  columnType: ColumnType!
  "JSON property to which the new column corresponds."
  jsonProperty: String
  "The error that occurred while adding the column data, if any."
  error: Error
  "The current progress of the AddColumnToDataPool Job, from 0.0 to 1.0."
  progress: Float!
  "The time at which the AddColumnToDataPool Job started."
  startedAt: DateTime
  "The time at which the AddColumnToDataPool Job succeeded."
  succeededAt: DateTime
  "The time at which the AddColumnToDataPool Job failed."
  failedAt: DateTime
}

"""
The fields for creating an Add Column Job.
"""
input CreateAddColumnToDataPoolJobInput {
  "The Data Pool to which the column will be added."
  dataPool: ID!
  "Name of the new column."
  columnName: String!
  "Type of the new column."
  columnType: ColumnType!
  "JSON property to which the new column corresponds."
  jsonProperty: String
}

"""
The response returned by the Add Column Job.
"""
type AddColumnToDataPoolJobResponse {
  "The AddColumnToDataPool Job that was just created."
  job: AddColumnToDataPoolJob!
}

"""
UpdateDataPoolRecords Job scheduled for a specific Data Pool.
The Update Data Pool Records Job represents the asynchronous process of updating records
given some filters, inside a Data Pool. It tracks the process of updating records
until it is finished, showing the progress and the outcome when it is finished.
"""
type UpdateDataPoolRecordsJob implements Node & Job {
  "The UpdateDataPoolRecords Job's ID"
  id: ID!
  "The UpdateDataPoolRecords Job's creation date and time in UTC"
  createdAt: DateTime!
  "Who created the UpdateDataPoolRecords Job"
  createdBy: String!
  "The UpdateDataPoolRecords Job's last modification date and time in UTC"
  modifiedAt: DateTime!
  "Who last modified the UpdateDataPoolRecords Job"
  modifiedBy: String!
  "Account to which the UpdateDataPoolRecords Job belongs"
  account: Account!
  "Environment to which the UpdateDataPoolRecords Job belongs"
  environment: Environment!
  "The Data Pool whose records will be updated by the UpdateDataPoolRecords Job"
  dataPool: DataPool!
  "The current UpdateDataPoolRecords Job's status"
  status: JobStatus!
  "The list of filters that will be used for updating data. Data matching the filters will be updated."
  filters: [Filter!]!
  "Describes how the job will update the records."
  set: [UpdateDataPoolRecordsJobSetColumn!]!
  "The error that occurred while updating data, if any."
  error: Error
  "The current progress of the UpdateDataPoolRecords Job, from 0.0 to 1.0."
  progress: Float!
  "The time at which the UpdateDataPoolRecords Job started."
  startedAt: DateTime
  "The time at which the UpdateDataPoolRecords Job succeeded."
  succeededAt: DateTime
  "The time at which the UpdateDataPoolRecords Job failed."
  failedAt: DateTime
}

"""
The fields for creating an Update Data Pool Records Job.
"""
input CreateUpdateDataPoolRecordsJobInput {
  "The Data Pool that is going to get its records updated."
  dataPool: ID!
  "The list of filters that will be used for updating records. Records matching these filters will be updated."
  filters: [FilterInput!]!
  "Describes how the job will update the records."
  set: [UpdateDataPoolRecordsJobSetColumnInput!]!
}

"""
The response returned by the Update Data Pool Records Job.
"""
type UpdateDataPoolRecordsJobResponse {
  "The UpdateDataPoolRecords Job that was just created."
  job: UpdateDataPoolRecordsJob!
}

type DataPoolAccessPolicy implements Node & Common {
  """
  The ID of the Data Pool Access Policy.
  """
  id: ID!
  """
  The Data Pool Access Policy's unique name.
  """
  uniqueName: String!
  """
  The Data Pool Access Policy's description.
  """
  description: String!
  """
  The Data Pool Access Policy's Account.
  """
  account: Account!
  """
  The Data Pool Access Policy's Environment.
  """
  environment: Environment!
  """
  The Data Pool Access Policy's creation date and time in UTC.
  """
  createdAt: DateTime!
  """
  The Data Pool Access Policy's last modification date and time in UTC.
  """
  modifiedAt: DateTime!
  """
  The Data Pool Access Policy's creator. It can be either a User ID, an Application ID, or "system" if it was created by Propel.
  """
  createdBy: String!
  """
  The Data Pool Access Policy's last modifier. It can be either a User ID, an Application ID, or "system" if it was modified by Propel.
  """
  modifiedBy: String!
  """
  The Data Pool to which the Access Policy belongs.
  """
  dataPool: DataPool!
  """
  Columns that the Access Policy makes available for querying.
  """
  columns: [String!]!
  """
  Row-level filters that the Access Policy applies before executing queries.
  """
  rows: [Filter!]!
  """
  Applications that are assigned to this Data Pool Access Policy.
  """
  applications(first: Int, after: String, last: Int, before: String): ApplicationConnection!
}

input CreateDataPoolAccessPolicyInput {
  """
  The Data Pool Access Policy's unique name. If not specified, Propel will set the ID as unique name.
  """
  uniqueName: String
  """
  The Data Pool Access Policy's description.
  """
  description: String
  """
  The Data Pool to which the Access Policy belongs.
  """
  dataPool: ID!
  """
  Columns that the Access Policy makes available for querying.

  If set to `["*"]`, all columns will be available for querying.
  """
  columns: [String!]!
  """
  Row-level filters that the Access Policy applies before executing queries.
  """
  rows: [FilterInput!]
}

type DataPoolAccessPolicyResponse {
  """
  The Data Pool Access Policy.
  """
  dataPoolAccessPolicy: DataPoolAccessPolicy!
}

input ModifyDataPoolAccessPolicyInput {
  id: ID!
  """
  The Data Pool Access Policy's new unique name.
  """
  uniqueName: String
  """
  The Data Pool Access Policy's new description.
  """
  description: String
  """
  Columns that the Access Policy makes available for querying. If not provided this property will not be modified.
  """
  columns: [String!]
  """
  Row-level filters that the Access Policy applies before executing queries. If not provided this property will not be modified.
  """
  rows: [FilterInput!]
}

type Mutation {
  """
  Creates a Data Pool Access Policy for the specified Data Pool.

  [Learn more about Data Pool Access Policy](https://www.propeldata.com/docs/control-access).
  """
  createDataPoolAccessPolicy(input: CreateDataPoolAccessPolicyInput!): DataPoolAccessPolicyResponse!
  """
  Modifies a Data Pool Access Policy with the provided unique name, description, columns and rows. If any of the optional arguments are omitted, those properties will be unchanged on the Data Pool Access Policy.

  [Learn more about Data Pool Access Policy](https://www.propeldata.com/docs/control-access).
  """
  modifyDataPoolAccessPolicy(input: ModifyDataPoolAccessPolicyInput!): DataPoolAccessPolicyResponse!
  """
  Deletes a Data Pool Access Policy by ID and returns its ID if the Data Pool Access Policy was deleted successfully.

  [Learn more about Data Pool Access Policy](https://www.propeldata.com/docs/control-access).
  """
  deleteDataPoolAccessPolicy(id: ID!): ID
  """
  Assign a Data Pool Access Policy to an Application.

  The Data Pool Access Policy will restrict the Data Pool rows and columns that the Application
  can query. If the Data Pool has `accessControlEnabled` set to true, the Application
  *must* have a Data Pool Access Policy assigned in order to query the Data Pool.

  An Application can have at most one Data Pool Access Policy assigned for a given Data Pool. If
  an Application already has a Data Pool Access Policy for a given Data Pool, and you call this
  mutation with another Data Pool Access Policy for the same Data Pool, the Application's Data Pool Access
  Policy will be replaced.
  """
  assignDataPoolAccessPolicyToApplication(dataPoolAccessPolicy: ID!, application: ID!): ID
  """
  Unassign a Data Pool Access Policy from an Application.

  Once unassigned, whether the Application will be able to query the Data Pool is
  controlled by the Data Pool's `accessControlEnabled` property. If
  `accessControlEnabled` is true, the Application will no longer be able to query the
  Data Pool. If `accessControlEnabled` is false, the Application will be able to query
  *all* data in the Data Pool, unrestricted.
  """
  unAssignDataPoolAccessPolicyFromApplication(dataPoolAccessPolicy: ID!, application: ID!): ID
  """
  Creates a new Application and returns the newly created Application (or an error message if creating the Application fails).

  [Learn more about Applications](https://www.propeldata.com/docs/applications).
  """
  createApplication(input: createApplicationInput!): ApplicationOrFailureResponse
  """
  Modifies an Application with the provided unique name, description, Propeller, and scopes. If any of the optional arguments are omitted, those properties will be unchanged on the Application.

  [Learn more about Applications](https://www.propeldata.com/docs/applications).
  """
  modifyApplication(input: modifyApplicationInput!): ApplicationOrFailureResponse
  """
  Deletes an Application by ID and returns its ID if the Application was deleted successfully.

  [Learn more about Applications](https://www.propeldata.com/docs/applications).
  """
  deleteApplication(id: ID!): ID
  """
  Deletes an Application by unique name and returns its ID if the Application was deleted successfully.

  [Learn more about Applications](https://www.propeldata.com/docs/applications).
  """
  deleteApplicationByName(uniqueName: String!): ID
  """
  Creates a new Data Source from the given Snowflake database using the specified Snowflake account, warehouse, schema, username, and role.

  Returns the newly created Data Source (or an error message if creating the Data Source fails).
  """
  createSnowflakeDataSource(input: createSnowflakeDataSourceInput!): DataSourceOrFailureResponse
  """
  Modifies a Data Source with the provided unique name, description, and connection settings. If any of the optional arguments are omitted, those properties will be unchanged on the Data Source.
  """
  modifySnowflakeDataSource(input: modifySnowflakeDataSourceInput!): DataSourceOrFailureResponse
  """
  Attempts to reconnect a Data Source. The mutation then returns the Data Source object.
  """
  reconnectDataSource(input: idOrUniqueName!): DataSource
  """
  Introspects the tables in a Data Source.

  Returns the tables along with when they were last cached from the Data Source.
  """
  introspectTables(input: idOrUniqueName!): TableIntrospection
  """
  Tests that Propel can actually connect to the data warehouse. Updates the status.
  """
  testDataSource(input: idOrUniqueName!): DataSourceOrFailureResponse
  """
  Deletes a Data Source by ID and returns its ID if the Data Source was deleted successfully.
  """
  deleteDataSource(id: ID!): ID
  """
  Deletes a Data Source by unique name and returns its ID if the Data Source was deleted successfully.
  """
  deleteDataSourceByName(uniqueName: String!): ID
  """
  Creates a new Data Pool from the given Data Source based on the specified table and using a particular column as the timestamp.

  Returns the newly created Data Pool (or an error message if creating the Data Pool fails).

  [Learn more about Data Pools](https://www.propeldata.com/docs/connect-your-data#key-concept-2-data-pools).
  """
  createDataPoolV2(input: CreateDataPoolInputV2!): DataPoolResponse
  """
  Modifies a Data Pool with the provided unique name, description, and data retention time. If any of the optional arguments are omitted, those properties will be unchanged on the Data Pool.
  """
  modifyDataPool(input: modifyDataPoolInput!): DataPoolOrFailureResponse
  """
  Deprecated. Use `retryDataPoolSetup` instead.
  """
  reconnectDataPool(input: idOrUniqueName!): DataPool @deprecated(reason: "Use `retryDataPoolSetup` instead")
  """
  Retries to set up the Data Pool identified by the given ID.
  """
  retryDataPoolSetup(id: ID!): DataPool
  """
  Retries to set up the Data Pool identified by the given unique name.
  """
  retryDataPoolSetupByName(uniqueName: String!): DataPool
  """
  Extracts the schema from the table and updates the schema object.
  """
  inspectDataPoolSchema(input: idOrUniqueName!): DataPoolOrFailureResponse
  """
  Tests that Propel has access to the Data Pool's table in its corresponding Data Source and will be able to Sync data. Updates the status.
  """
  testDataPool(input: idOrUniqueName!): DataPoolOrFailureResponse
  """
  Deletes a Data Pool by ID and returns its ID if the Data Pool was deleted successfully.
  """
  deleteDataPool(id: ID!): ID
  """
  Deletes a Data Pool by unique name and returns its ID if the Data Pool was deleted successfully.
  """
  deleteDataPoolByName(uniqueName: String!): ID
  """
  Disables syncing of a Data Pool.
  """
  disableSyncing(id: ID!): DataPool
  """
  Re-enables syncing of a Data Pool.
  """
  enableSyncing(id: ID!): DataPool
  """
  Creates a new Count Metric from the given Data Pool and returns the newly created Metric (or an error message if creating the Metric fails).

  [Learn more about Metrics](https://www.propeldata.com/docs/metrics).
  """
  createCountMetric(input: CreateCountMetricInput): MetricResponse
  """
  Creates a new Count Distinct Metric from the given Data Pool and returns the newly created Metric (or an error message if creating the Metric fails).

  [Learn more about Metrics](https://www.propeldata.com/docs/metrics).
  """
  createCountDistinctMetric(input: CreateCountDistinctMetricInput): MetricResponse
  """
  Creates a new Sum Metric from the given Data Pool and returns the newly created Metric (or an error message if creating the Metric fails).

  [Learn more about Metrics](https://www.propeldata.com/docs/metrics).
  """
  createSumMetric(input: CreateSumMetricInput): MetricResponse
  """
  Creates a new Average Metric from the given Data Pool and returns the newly created Metric (or an error message if creating the Metric fails).

  [Learn more about Metrics](https://www.propeldata.com/docs/metrics).
  """
  createAverageMetric(input: CreateAverageMetricInput): MetricResponse
  """
  Creates a new Min Metric from the given Data Pool and returns the newly created Metric (or an error message if creating the Metric fails).

  [Learn more about Metrics](https://www.propeldata.com/docs/metrics).
  """
  createMinMetric(input: CreateMinMetricInput): MetricResponse
  """
  Creates a new Max Metric from the given Data Pool and returns the newly created Metric (or an error message if creating the Metric fails).

  [Learn more about Metrics](https://www.propeldata.com/docs/metrics).
  """
  createMaxMetric(input: CreateMaxMetricInput): MetricResponse
  """
  Creates a new Custom Metric from the given Data Pool and returns the newly created Metric (or an error message if creating the Metric fails).

  [Learn more about Metrics](https://www.propeldata.com/docs/metrics).
  """
  createCustomMetric(input: CreateCustomMetricInput): MetricResponse
  """
  Modifies a Metric by ID with the provided unique name, description, and Dimensions. If any of the optional arguments are omitted, those properties will be unchanged on the Metric.
  """
  modifyMetric(input: ModifyMetricInput): MetricResponse
  """
  Migrates a Metric from one Data Pool to another.
  """
  migrateMetric(input: MigrateMetricInput!): Metric
  """
  Deletes a Metric by ID and returns its ID if the Metric was deleted successfully.
  """
  deleteMetric(id: ID!): ID
  """
  Deletes a Metric by unique name and returns its ID if the Metric was deleted successfully.
  """
  deleteMetricByName(uniqueName: String!): ID
  """
  Creates a new Booster for the given Metric and returns the newly created Booster.

  A Booster significantly improves the query performance for a Metric.
  """
  createBooster(input: CreateBoosterInput!): BoosterResponse
  """
  Deletes a Booster by ID and then returns the same ID if the Booster was deleted successfully.

  A Booster significantly improves the query performance for a Metric.
  """
  deleteBooster(id: ID!): ID
  """
  Creates a new Policy granting an Application access to a Metric's data.
  """
  createPolicy(input: CreatePolicyInput!): PolicyResponse @deprecated(reason: "Use Data Pool Access Policies instead")
  """
  Modifies an existing Policy. You can modify the Application's level of access to the Metric's data.
  """
  modifyPolicy(input: ModifyPolicyInput!): PolicyResponse @deprecated(reason: "Use Data Pool Access Policies instead")
  """
  Deletes a Policy. The associated Application will no longer have access to the Metric's data.
  """
  deletePolicy(id: ID!): ID @deprecated(reason: "Use Data Pool Access Policies instead")
  """
  Schedules a new Deletion Job on the specified Data Pool.
  """
  requestDelete(input: DeletionRequestInput!): RequestDeleteResponse!
    @deprecated(reason: "This has been renamed to `createDeletionJob`")
  """
  Schedules a new Deletion Job on the specified Data Pool.
  """
  createDeletionJob(input: CreateDeletionJobInput!): DeletionJobResponse!
  """
  Schedules a new AddColumnToDataPoolJob on the specified Data Pool.
  """
  createAddColumnToDataPoolJob(input: CreateAddColumnToDataPoolJobInput!): AddColumnToDataPoolJobResponse!
  """
  Schedules a new UpdateDataPoolRecords Job on the specified Data Pool.
  """
  createUpdateDataPoolRecordsJob(input: CreateUpdateDataPoolRecordsJobInput!): UpdateDataPoolRecordsJobResponse!
  """
  Manually trigger a Sync for a Data Pool.
  """
  syncDataPool(dataPoolId: ID!): Sync
  """
  Manually trigger a re-Sync for a Data Pool.
  """
  resyncDataPool(dataPoolId: ID!): Sync
  """
  Creates a new HTTP Data Source from the given settings.

  Returns the newly created Data Source (or an error message if creating the Data Source fails).
  """
  createHttpDataSource(input: CreateHttpDataSourceInput!): DataSourceResponse!
  """
  This mutation selects a Data Source by its ID or unique name and modifies it to have the given unique name, description, and connection settings.

  If any of the optional arguments are omitted, those properties will be unchanged on the Data Source.
  """
  modifyHttpDataSource(input: ModifyHttpDataSourceInput!): DataSourceResponse!
  """
  Creates a new Amazon S3 Data Source pointed at the specified S3 bucket.

  Returns the newly created Data Source (or an error message if creating the Data Source fails).
  """
  createS3DataSource(input: CreateS3DataSourceInput!): DataSourceResponse!
  """
  This mutation selects a Data Source by its ID or unique name and modifies it to have the given unique name, description, and connection settings.

  If any of the optional arguments are omitted, those properties will be unchanged on the Data Source.
  """
  modifyS3DataSource(input: ModifyS3DataSourceInput!): DataSourceResponse!
  """
  Creates a new Webhook Data Source from the given settings.

  Returns the newly created Data Source (or an error message if creating the Data Source fails).
  """
  createWebhookDataSource(input: CreateWebhookDataSourceInput!): DataSourceResponse!
  """
  Modifies the Data Source by the ID or unique name provided with the given unique name, description, and connection settings.

  If any of the optional arguments are omitted, those properties will be unchanged on the Data Source.
  """
  modifyWebhookDataSource(input: ModifyWebhookDataSourceInput!): DataSourceResponse!
}

"""
The Data Grid connection.

It includes `headers` and `rows` for a single page of a Data Grid table. It also allows paging forward and backward to other
pages of the Data Grid table.

Learn more about [pagination in GraphQL](https://www.propeldata.com/docs/api/pagination).
"""
type DataGridConnection {
  """
  The Data Grid table's headers.
  """
  headers: [String!]!
  """
  An array of arrays containing the values of the Data Grid table's rows.
  """
  rows: [[String]!]!
  """
  The Query statistics and metadata.
  """
  query: QueryInfo!
  """
  The Data Grid table's page info.
  """
  pageInfo: PageInfo!
  """
  The Data Grid table's edges.
  """
  edges: [DataGridEdge!]!
  """
  The Data Grid table's nodes.
  """
  nodes: [DataGridNode!]!
}

"""
The Data Grid edge object.

Learn more about [pagination in GraphQL](https://www.propeldata.com/docs/api/pagination).
"""
type DataGridEdge {
  """
  The edge's node.
  """
  node: DataGridNode!
  """
  The edge's cursor.
  """
  cursor: String!
}

"""
The Data Grid table's node.

This type represents a single row of a Data Grid table.

Learn more about [pagination in GraphQL](https://www.propeldata.com/docs/api/pagination).
"""
type DataGridNode {
  """
  The Data Grid table's headers.
  """
  headers: [String!]!
  """
  An array of the values for the row.
  """
  row: [String]!
}

"""
The fields for querying records by unique ID.
"""
input RecordsByUniqueIdInput {
  """
  Optionally specifies the Propeller to use. Applications may not set this value. Instead, Application Queries always use the Propeller configured on the Application.
  """
  propeller: Propeller
  """
  The Data Pool to be queried. If a Data Pool ID is provided it will take precedence over the Data Pool name.
  """
  dataPoolId: ID
  """
  The Data Pool to be queried. If a Data Pool ID is provided it will take precedence over the Data Pool name.
  """
  dataPoolName: String
  """
  The columns to retrieve.
  """
  columns: [String!]!
  """
  The unique IDs of the records to retrieve.
  """
  uniqueIds: [String!]!
}

type RecordsByUniqueIdResponse {
  """
  The Data Pool columns for the record.
  """
  columns: [String!]!
  """
  An array of values for the record.
  """
  values: [[String]!]!
  """
  The Query statistics and metadata.
  """
  query: QueryInfo!
}

"""
The fields for querying the top values in a given column.
"""
input TopValuesInput {
  """
  The Data Pool to be queried. A Data Pool ID or unique name can be provided.
  """
  dataPool: DataPoolInput!
  """
  The column to fetch the unique values from.
  """
  columnName: String!
  """
  The time range for calculating the top values.
  """
  timeRange: TimeRangeInput!
  """
  The time zone to use. Dates and times are always returned in UTC, but setting the time zone influences relative time ranges and granularities.

  You can set this to "America/Los_Angeles", "Europe/Berlin", or any other value in the [IANA time zone database](https://en.wikipedia.org/wiki/Tz_database). Defaults to "UTC".
  """
  timeZone: String
  """
  The maximum number of values to return. It can be a number between 1 and 1,000. If the parameter is omitted, default value 10 is used.
  """
  maxValues: Int
}

type TopValuesResponse {
  """
  An array with the list of values.
  """
  values: [String!]!
}

"""
Input to the SqlV1 api.
"""
input SqlV1Input {
  """
  The SQL query.
  """
  query: String!

  """
  Optionally specifies the Propeller to use. Applications may not set this value. Instead, Application Queries always use the Propeller configured on the Application.
  """
  propeller: Propeller @tag(name: "internal")

  """
  query timeout in milliseconds.
  """
  timeout: Int @tag(name: "internal")
}

type SqlColumnResponse {
  """
  The name of the returned column.
  """
  columnName: String!

  """
  The returned column's type.
  """
  type: ColumnType!

  """
  Whether the column is nullable, meaning whether it accepts a null value.
  """
  isNullable: Boolean!
}

"""
Response from the SQL API.
"""
type SqlResponse {
  """
  The column names in the same order as present in the `data` field.
  """
  columns: [SqlColumnResponse!]!
  """
  The data gathered by the SQL query. The data is returned in an N x M matrix format, where the
  first dimension are the rows retrieved, and the second dimension are the columns. Each cell
  can be either a string or null, and the string can represent a number, text, date or boolean value.
  """
  rows: [[String]!]!
  """
  The Query statistics and metadata.
  """
  info: QueryInfo!
}

"""
Input to the SqlV1 api.
"""
input SqlV1Input {
  """
  The SQL query.
  """
  query: String!

  """
  Optionally specifies the Propeller to use. Applications may not set this value. Instead, Application Queries always use the Propeller configured on the Application.
  """
  propeller: Propeller @tag(name: "internal")

  """
  query timeout in milliseconds.
  """
  timeout: Int @tag(name: "internal")
}

type SqlColumnResponse {
  """
  The name of the returned column.
  """
  columnName: String!

  """
  The returned column's type.
  """
  type: ColumnType!

  """
  Whether the column is nullable, meaning whether it accepts a null value.
  """
  isNullable: Boolean!
}

"""
Response from the SQL API.
"""
type SqlResponse {
  """
  The column names in the same order as present in the `data` field.
  """
  columns: [SqlColumnResponse!]!
  """
  The data gathered by the SQL query. The data is returned in an N x M matrix format, where the
  first dimension are the rows retrieved, and the second dimension are the columns. Each cell
  can be either a string or null, and the string can represent a number, text, date or boolean value.
  """
  rows: [[String]!]!
  """
  The Query statistics and metadata.
  """
  info: QueryInfo!
}

type Query {
  """
  This query returns the Application specified by the given ID.

  [Learn more about Applications](https://www.propeldata.com/docs/applications).
  """
  application(id: ID!): Application
  """
  This query returns the Application with the given unique name.

  [Learn more about Applications](https://www.propeldata.com/docs/applications).
  """
  applicationByName(uniqueName: String!): Application
  """
  This query returns the Applications within the Environment.

  [Learn more about Applications](https://www.propeldata.com/docs/applications).

  The `applications` query uses [cursor-based pagination](/docs/api/pagination) typical of GraphQL APIs. You can use the pairs of parameters `first` and `after` or `last` and `before` to page forward or backward through the results, respectively.

  For forward pagination, the `first` parameter defines the number of results to return, and the `after` parameter defines the cursor to continue from. You should pass the cursor for the _last_ result of the current page to `after`.

  For backward pagination, the `last` parameter defines the number of results to return, and the `before` parameter defines the cursor to continue from. You should pass the cursor for the _first_ result of the current page to `before`.
  """
  applications(first: Int, after: String, last: Int, before: String): ApplicationConnection
  """
  This query returns the Data Source specified by the given ID.

  A Data Source is a connection to your data warehouse. It has the necessary connection details for Propel to access Snowflake or any other supported Data Source.
  """
  dataSource(id: ID!): DataSource
  """
  This query returns the Data Source specified by the given unique name.

  A Data Source is a connection to your data warehouse. It has the necessary connection details for Propel to access Snowflake or any other supported Data Source.
  """
  dataSourceByName(uniqueName: String!): DataSource
  """
  This query returns the Data Sources within the Environment.

  A Data Source is a connection to your data warehouse. It has the necessary connection details for Propel to access Snowflake or any other supported Data Source. Environments are independent and isolated Propel workspaces for development, staging (testing), and production workloads.

  The `dataSources` query uses [cursor-based pagination](/docs/api/pagination) typical of GraphQL APIs. You can use the pairs of parameters `first` and `after` or `last` and `before` to page forward or backward through the results, respectively.

  For forward pagination, the `first` parameter defines the number of results to return, and the `after` parameter defines the cursor to continue from. You should pass the cursor for the _last_ result of the current page to `after`.

  For backward pagination, the `last` parameter defines the number of results to return, and the `before` parameter defines the cursor to continue from. You should pass the cursor for the _first_ result of the current page to `before`.
  """
  dataSources(first: Int, after: String, last: Int, before: String): DataSourceConnection
  """
  This query returns the Data Pool specified by the given ID.

  A Data Pool is a cached table hydrated from your data warehouse optimized for high-concurrency and low-latency queries.
  """
  dataPool(id: ID!): DataPool
  """
  This query returns the Data Pool specified by the given unique name.

  A Data Pool is a cached table hydrated from your data warehouse optimized for high-concurrency and low-latency queries.
  """
  dataPoolByName(uniqueName: String!): DataPool
  """
  This query returns the Data Pools within the Environment.

  A Data Pool is a cached table hydrated from your data warehouse optimized for high-concurrency and low-latency queries. Environments are independent and isolated Propel workspaces for development, staging (testing), and production workloads.

  The `dataPools` query uses [cursor-based pagination](/docs/api/pagination) typical of GraphQL APIs. You can use the pairs of parameters `first` and `after` or `last` and `before` to page forward or backward through the results, respectively.

  For forward pagination, the `first` parameter defines the number of results to return, and the `after` parameter defines the cursor to continue from. You should pass the cursor for the _last_ result of the current page to `after`.

  For backward pagination, the `last` parameter defines the number of results to return, and the `before` parameter defines the cursor to continue from. You should pass the cursor for the _first_ result of the current page to `before`.
  """
  dataPools(first: Int, after: String, last: Int, before: String): DataPoolConnection
  """
  This query returns the Data Pool Access Policy specified by the given ID.

  A Data Pool Access Policy limits the data that Applications can access within a Data Pool.
  """
  dataPoolAccessPolicy(id: ID!): DataPoolAccessPolicy
  """
  This query returns the Metric specified by the given ID.

  A Metric is a business indicator measured over time.
  """
  metric(id: ID!): Metric
  """
  This query returns the Metric specified by the given unique name.

  A Metric is a business indicator measured over time.
  """
  metricByName(uniqueName: String!): Metric
  """
  This query returns the Metrics within the Environment.

  A Metric is a business indicator measured over time. Each Metric is associated with one Data Pool, which is a cached table hydrated from your data warehouse optimized for high-concurrency and low-latency queries. Environments are independent and isolated Propel workspaces for development, staging (testing), and production workloads.

  The `metrics` query uses [cursor-based pagination](/docs/api/pagination) typical of GraphQL APIs. You can use the pairs of parameters `first` and `after` or `last` and `before` to page forward or backward through the results, respectively.

  For forward pagination, the `first` parameter defines the number of results to return, and the `after` parameter defines the cursor to continue from. You should pass the cursor for the _last_ result of the current page to `after`.

  For backward pagination, the `last` parameter defines the number of results to return, and the `before` parameter defines the cursor to continue from. You should pass the cursor for the _first_ result of the current page to `before`.
  """
  metrics(first: Int, after: String, last: Int, before: String): MetricConnection
  """
  This query returns the Booster specified by the given ID.

  A Booster significantly improves the query performance for a Metric.
  """
  booster(id: ID!): Booster
  """
  Returns a Policy by ID.
  """
  policy(id: ID!): Policy
  """
  Returns a Sync by ID.
  """
  sync(id: ID!): Sync
  """
  Returns a table by ID.
  """
  table(id: ID!): Table
  """
  Build a report, or table, consisting of multiple Metrics broken down by one-or-more dimensions.

  The first few columns of the report are the dimensions you choose to break down by. The subsequent columns are the
  Metrics you choose to query. By default, the report sorts on the first Metric in descending order, but you can
  configure this with the `orderByMetric` and `sort` inputs.

  Finally, reports use [cursor-based pagination](/docs/api/pagination). You can control page size with the `first` and
  `last` inputs.
  """
  metricReport(input: MetricReportInput!): MetricReportConnection
  """
  This query returns the individual records of a Data Pool with the convenience of built-in pagination, filtering, and sorting.
  """
  dataGrid(input: DataGridInput!): DataGridConnection!
  """
  This query returns records by the given unique IDs.
  """
  recordsByUniqueId(input: RecordsByUniqueIdInput!): RecordsByUniqueIdResponse!
  """
  This query returns records by the given unique IDs.
  """
  recordsByUniqueId(input: RecordsByUniqueIdInput!): RecordsByUniqueIdResponse!
  """
  This query returns an array of the most frequent values in a given column. The resulting array is sorted in descending order of approximate frequency of values.
  """
  topValues(input: TopValuesInput!): TopValuesResponse!
  """
  Query a Metric in counter format. A single Metric value for the given time range and filters.
  """
  counter(input: CounterInput!): CounterResponse
  """
  Query a Metric in time series format. Arrays of timestamps and Metric values for the given time range and filters.
  """
  timeSeries(input: TimeSeriesInput!): TimeSeriesResponse
  """
  Query a Metric in leaderboard format. A table (array of rows) with the selected dimensions and corresponding Metric values for the given time range and filters.
  """
  leaderboard(input: LeaderboardInput!): LeaderboardResponse
  """
  This query returns the Deletion Job specified by the given ID.

  The Deletion Job represents the asynchronous process of deleting data
  given some filters inside a Data Pool.
  """
  deletionJob(id: ID!): DeletionJob
  """
  This query returns the AddColumnToDataPoolJob specified by the given ID.

  The AddColumnToDataPoolJob represents the asynchronous process of adding
  a column, given its name and type, to a Data Pool.
  """
  addColumnToDataPoolJob(id: ID!): AddColumnToDataPoolJob
  """
  This query returns the AddColumnToDataPool Job specified by a given status.
  """
  addColumnToDataPoolJobByStatus(
    status: JobStatus!
    first: Int
    after: String
    last: Int
    before: String
  ): AddColumnToDataPoolJobConnection!
  """
  This query returns the UpdateDataPoolRecords Job specified by the given ID.

  The UpdateDataPoolRecords Job represents the asynchronous process of updating
  records inside a Data Pool.
  """
  updateDataPoolRecordsJob(id: ID!): UpdateDataPoolRecordsJob
  """
  This query returns the UpdateDataPoolRecords Job specified by a given status.
  """
  updateDataPoolRecordsJobByStatus(
    status: JobStatus!
    first: Int
    after: String
    last: Int
    before: String
  ): UpdateDataPoolRecordsJobConnection!

  """
  Query Data Pools using SQL.
  """
  sqlV1(input: SqlV1Input!): SqlResponse!
}

"""
The Add column to Data Pool Job connection object.

Learn more about [pagination in GraphQL](https://www.propeldata.com/docs/api/pagination).
"""
type AddColumnToDataPoolJobConnection {
  """
  The Add column to Data Pool Job connection's edges.
  """
  edges: [AddColumnToDataPoolJobEdge!]!
  """
  The Add column to Data Pool Job connection's nodes.
  """
  nodes: [AddColumnToDataPoolJob!]!
  """
  The Add column to Data Pool Job connection's page info.
  """
  pageInfo: PageInfo!
}

"""
The Add column to Data Pool Job edge object.

Learn more about [pagination in GraphQL](https://www.propeldata.com/docs/api/pagination).
"""
type AddColumnToDataPoolJobEdge {
  """
  The edge's cursor.
  """
  cursor: String!
  """
  The edge's node.
  """
  node: AddColumnToDataPoolJob!
}

"""
The Application connection object.

Learn more about [pagination in GraphQL](https://www.propeldata.com/docs/api/pagination).
"""
type ApplicationConnection {
  """
  The Application connection's edges.
  """
  edges: [ApplicationEdge!]!
  """
  The Application connection's nodes.
  """
  nodes: [Application!]!
  """
  The Application connection's page info.
  """
  pageInfo: PageInfo!
}

"""
The Application edge object.

Learn more about [pagination in GraphQL](https://www.propeldata.com/docs/api/pagination).
"""
type ApplicationEdge {
  """
  The edge's cursor.
  """
  cursor: String!
  """
  The edge's node.
  """
  node: Application!
}

"""
The Booster connection object.

Learn more about [pagination in GraphQL](https://www.propeldata.com/docs/api/pagination).
"""
type BoosterConnection {
  """
  The Booster connection's edges.
  """
  edges: [BoosterEdge!]!
  """
  The Booster connection's nodes.
  """
  nodes: [Booster!]!
  """
  The Booster connection's page info.
  """
  pageInfo: PageInfo!
}

"""
The Booster edge object.

Learn more about [pagination in GraphQL](https://www.propeldata.com/docs/api/pagination).
"""
type BoosterEdge {
  """
  The edge's cursor.
  """
  cursor: String!
  """
  The edge's node.
  """
  node: Booster!
}

"""
The column connection object.

Learn more about [pagination in GraphQL](https://www.propeldata.com/docs/api/pagination).
"""
type ColumnConnection {
  """
  The time at which the columns were cached (i.e., the time at which they were introspected).
  """
  cachedAt: DateTime!
  """
  The column connection's edges.
  """
  edges: [ColumnEdge!]!
  """
  The column connection's nodes.
  """
  nodes: [Column!]!
  """
  The column connection's page info.
  """
  pageInfo: PageInfo!
}

"""
The column edge object.

Learn more about [pagination in GraphQL](https://www.propeldata.com/docs/api/pagination).
"""
type ColumnEdge {
  """
  The edge's cursor.
  """
  cursor: String!
  """
  The edge's node.
  """
  node: Column!
}

"""
The Data Pool Access Policy connection object.

Learn more about [pagination in GraphQL](https://www.propeldata.com/docs/api/pagination).
"""
type DataPoolAccessPolicyConnection {
  """
  The Data Pool Access Policy connection's edges.
  """
  edges: [DataPoolAccessPolicyEdge!]!
  """
  The Data Pool Access Policy connection's nodes.
  """
  nodes: [DataPoolAccessPolicy!]!
  """
  The Data Pool Access Policy connection's page info.
  """
  pageInfo: PageInfo!
}

"""
The Data Pool Access Policy edge object.

Learn more about [pagination in GraphQL](https://www.propeldata.com/docs/api/pagination).
"""
type DataPoolAccessPolicyEdge {
  """
  The edge's cursor.
  """
  cursor: String!
  """
  The edge's node.
  """
  node: DataPoolAccessPolicy!
}

"""
The Data Pool column connection object.

Learn more about [pagination in GraphQL](https://www.propeldata.com/docs/api/pagination).
"""
type DataPoolColumnConnection {
  """
  The Data Pool column connection's edges.
  """
  edges: [DataPoolColumnEdge!]!
  """
  The Data Pool column connection's nodes.
  """
  nodes: [DataPoolColumn!]!
  """
  The Data Pool column connection's page info.
  """
  pageInfo: PageInfo!
}

"""
The Data Pool column edge object.

Learn more about [pagination in GraphQL](https://www.propeldata.com/docs/api/pagination).
"""
type DataPoolColumnEdge {
  """
  The edge's cursor.
  """
  cursor: String!
  """
  The edge's node.
  """
  node: DataPoolColumn!
}

"""
The Data Pool connection object.

Learn more about [pagination in GraphQL](https://www.propeldata.com/docs/api/pagination).
"""
type DataPoolConnection {
  """
  The Data Pool connection's edges.
  """
  edges: [DataPoolEdge!]!
  """
  The Data Pool connection's nodes.
  """
  nodes: [DataPool!]!
  """
  The Data Pool connection's page info.
  """
  pageInfo: PageInfo!
}

"""
The Data Pool edge object.

Learn more about [pagination in GraphQL](https://www.propeldata.com/docs/api/pagination).
"""
type DataPoolEdge {
  """
  The edge's cursor.
  """
  cursor: String!
  """
  The edge's node.
  """
  node: DataPool!
}

"""
The Data Source connection object.

Learn more about [pagination in GraphQL](https://www.propeldata.com/docs/api/pagination).
"""
type DataSourceConnection {
  """
  The Data Source connection's edges.
  """
  edges: [DataSourceEdge!]!
  """
  The Data Source connection's nodes.
  """
  nodes: [DataSource!]!
  """
  The Data Source connection's page info.
  """
  pageInfo: PageInfo!
}

"""
The Data Source edge object.

Learn more about [pagination in GraphQL](https://www.propeldata.com/docs/api/pagination).
"""
type DataSourceEdge {
  """
  The edge's cursor.
  """
  cursor: String!
  """
  The edge's node.
  """
  node: DataSource!
}

"""
The Deletion Job connection object.

Learn more about [pagination in GraphQL](https://www.propeldata.com/docs/api/pagination).
"""
type DeletionJobConnection {
  """
  The Deletion Job connection's edges.
  """
  edges: [DeletionJobEdge!]!
  """
  The Deletion Job connection's nodes.
  """
  nodes: [DeletionJob!]!
  """
  The Deletion Job connection's page info.
  """
  pageInfo: PageInfo!
}

"""
The Deletion Job edge object.

Learn more about [pagination in GraphQL](https://www.propeldata.com/docs/api/pagination).
"""
type DeletionJobEdge {
  """
  The edge's cursor.
  """
  cursor: String!
  """
  The edge's node.
  """
  node: DeletionJob!
}

"""
The Metric connection object.

Learn more about [pagination in GraphQL](https://www.propeldata.com/docs/api/pagination).
"""
type MetricConnection {
  """
  The Metric connection's edges.
  """
  edges: [MetricEdge!]!
  """
  The Metric connection's nodes.
  """
  nodes: [Metric!]!
  """
  The Metric connection's page info.
  """
  pageInfo: PageInfo!
}

"""
The Metric edge object.

Learn more about [pagination in GraphQL](https://www.propeldata.com/docs/api/pagination).
"""
type MetricEdge {
  """
  The edge's cursor.
  """
  cursor: String!
  """
  The edge's node.
  """
  node: Metric!
}

"""
The Policy connection object.

Learn more about [pagination in GraphQL](https://www.propeldata.com/docs/api/pagination).
"""
type PolicyConnection {
  """
  The Policy connection's edges.
  """
  edges: [PolicyEdge!]!
  """
  The Policy connection's nodes.
  """
  nodes: [Policy!]!
  """
  The Policy connection's page info.
  """
  pageInfo: PageInfo!
}

"""
The Policy edge object.

Learn more about [pagination in GraphQL](https://www.propeldata.com/docs/api/pagination).
"""
type PolicyEdge {
  """
  The edge's cursor.
  """
  cursor: String!
  """
  The edge's node.
  """
  node: Policy!
}

"""
The Sync connection object.

Learn more about [pagination in GraphQL](https://www.propeldata.com/docs/api/pagination).
"""
type SyncConnection {
  """
  The Sync connection's edges.
  """
  edges: [SyncEdge!]!
  """
  The Sync connection's nodes.
  """
  nodes: [Sync!]!
  """
  The Sync connection's page info.
  """
  pageInfo: PageInfo!
}

"""
The Sync edge object.

Learn more about [pagination in GraphQL](https://www.propeldata.com/docs/api/pagination).
"""
type SyncEdge {
  """
  The edge's cursor.
  """
  cursor: String!
  """
  The edge's node.
  """
  node: Sync!
}

"""
The table connection object.

Learn more about [pagination in GraphQL](https://www.propeldata.com/docs/api/pagination).
"""
type TableConnection {
  """
  The time at which the tables were cached (i.e., the time at which they were introspected).
  """
  cachedAt: DateTime!
  """
  The table connection's edges.
  """
  edges: [TableEdge!]!
  """
  The table connection's nodes.
  """
  nodes: [Table!]!
  """
  The table connection's page info.
  """
  pageInfo: PageInfo!
}

"""
The table edge object.

Learn more about [pagination in GraphQL](https://www.propeldata.com/docs/api/pagination).
"""
type TableEdge {
  """
  The edge's cursor.
  """
  cursor: String!
  """
  The edge's node.
  """
  node: Table!
}

"""
The table introspection connection object.

Learn more about [pagination in GraphQL](https://www.propeldata.com/docs/api/pagination).
"""
type TableIntrospectionConnection {
  """
  The table introspection connection's edges.
  """
  edges: [TableIntrospectionEdge!]!
  """
  The table introspection connection's nodes.
  """
  nodes: [TableIntrospection!]!
  """
  The table introspection connection's page info.
  """
  pageInfo: PageInfo!
}

"""
The table introspection edge object.

Learn more about [pagination in GraphQL](https://www.propeldata.com/docs/api/pagination).
"""
type TableIntrospectionEdge {
  """
  The edge's cursor.
  """
  cursor: String!
  """
  The edge's node.
  """
  node: TableIntrospection!
}

"""
The Update Data Pool records Job connection object.

Learn more about [pagination in GraphQL](https://www.propeldata.com/docs/api/pagination).
"""
type UpdateDataPoolRecordsJobConnection {
  """
  The Update Data Pool records Job connection's edges.
  """
  edges: [UpdateDataPoolRecordsJobEdge!]!
  """
  The Update Data Pool records Job connection's nodes.
  """
  nodes: [UpdateDataPoolRecordsJob!]!
  """
  The Update Data Pool records Job connection's page info.
  """
  pageInfo: PageInfo!
}

"""
The Update Data Pool records Job edge object.

Learn more about [pagination in GraphQL](https://www.propeldata.com/docs/api/pagination).
"""
type UpdateDataPoolRecordsJobEdge {
  """
  The edge's cursor.
  """
  cursor: String!
  """
  The edge's node.
  """
  node: UpdateDataPoolRecordsJob!
}

input CreateHttpDataSourceInput {
  """
  The HTTP Data Source's connection settings
  """
  connectionSettings: HttpConnectionSettingsInput!
  """
  The HTTP Data Source's description.
  """
  description: String
  """
  The HTTP Data Source's unique name. If not specified, Propel will set the ID as unique name.
  """
  uniqueName: String
}

"""
The HTTP Data Source connection settings.
"""
type HttpConnectionSettings {
  """
  The HTTP Basic authentication settings for uploading new data.

  If this parameter is not provided, anyone with the URL to your tables will be able to upload data. While it's OK to test without HTTP Basic authentication, we recommend enabling it.
  """
  basicAuth: HttpBasicAuthSettings
  """
  The HTTP Data Source's tables.
  """
  tables: [HttpDataSourceTable!]!
}

"""
The HTTP Data Source connection settings.
"""
input HttpConnectionSettingsInput {
  """
  The HTTP Basic authentication settings for uploading new data.

  If this parameter is not provided, anyone with the URL to your tables will be able to upload data. While it's OK to test without HTTP Basic authentication, we recommend enabling it.
  """
  basicAuth: HttpBasicAuthInput
  """
  The HTTP Data Source's tables.
  """
  tables: [HttpDataSourceTableInput!]!
}

input ModifyHttpDataSourceInput {
  """
  The HTTP Data Source's new connection settings. If not provided this property will not be modified.
  """
  connectionSettings: PartialHttpConnectionSettingsInput
  """
  The HTTP Data Source's new description. If not provided this property will not be modified.
  """
  description: String
  """
  The ID or unique name of the HTTP Data Source to modify.
  """
  idOrUniqueName: idOrUniqueName!
  """
  The HTTP Data Source's new unique name. If not provided this property will not be modified.
  """
  uniqueName: String
}

"""
The HTTP Data Source connection settings.
"""
input PartialHttpConnectionSettingsInput {
  """
  The HTTP Basic authentication settings for uploading new data.

  If this parameter is not provided, anyone with the URL to your tables will be able to upload data. While it's OK to test without HTTP Basic authentication, we recommend enabling it. If not provided this property will not be modified.
  """
  basicAuth: HttpBasicAuthInput
  """
  Set this to `false` to disable HTTP Basic authentication. Any previously stored HTTP Basic authentication settings will be cleared out. If not provided this property will not be modified.
  """
  basicAuthEnabled: Boolean
  """
  The HTTP Data Source's tables. If not provided this property will not be modified.
  """
  tables: [HttpDataSourceTableInput!]
}

input CreateS3DataSourceInput {
  """
  The S3 Data Source's connection settings
  """
  connectionSettings: S3ConnectionSettingsInput!
  """
  The S3 Data Source's description.
  """
  description: String
  """
  The S3 Data Source's unique name. If not specified, Propel will set the ID as unique name.
  """
  uniqueName: String
}

input ModifyS3DataSourceInput {
  """
  The S3 Data Source's new connection settings. If not provided this property will not be modified.
  """
  connectionSettings: PartialS3ConnectionSettingsInput
  """
  The S3 Data Source's new description. If not provided this property will not be modified.
  """
  description: String
  """
  The ID or unique name of the S3 Data Source to modify.
  """
  idOrUniqueName: idOrUniqueName!
  """
  The S3 Data Source's new unique name. If not provided this property will not be modified.
  """
  uniqueName: String
}

"""
The connection settings for an S3 Data Source. These include the S3 bucket name, the AWS access key ID, and the tables (along with their paths). We do not allow fetching the AWS secret access key after it has been set.
"""
input PartialS3ConnectionSettingsInput {
  """
  The AWS access key ID for an IAM user with sufficient access to the S3 bucket. If not provided this property will not be modified.
  """
  awsAccessKeyId: String
  """
  The AWS secret access key for an IAM user with sufficient access to the S3 bucket. If not provided this property will not be modified.
  """
  awsSecretAccessKey: String
  """
  The name of the S3 bucket. If not provided this property will not be modified.
  """
  bucket: String
  """
  The S3 Data Source's tables. If not provided this property will not be modified.
  """
  tables: [S3DataSourceTableInput!]
}

"""
The connection settings for an S3 Data Source. These include the S3 bucket name, the AWS access key ID, and the tables (along with their paths). We do not allow fetching the AWS secret access key after it has been set.
"""
type S3ConnectionSettings {
  """
  The AWS access key ID for an IAM user with sufficient access to the S3 bucket.
  """
  awsAccessKeyId: String!
  """
  The name of the S3 bucket.
  """
  bucket: String!
  """
  The S3 Data Source's tables.
  """
  tables: [S3DataSourceTable!]!
}

"""
The connection settings for an S3 Data Source. These include the S3 bucket name, the AWS access key ID, and the tables (along with their paths). We do not allow fetching the AWS secret access key after it has been set.
"""
input S3ConnectionSettingsInput {
  """
  The AWS access key ID for an IAM user with sufficient access to the S3 bucket.
  """
  awsAccessKeyId: String!
  """
  The AWS secret access key for an IAM user with sufficient access to the S3 bucket.
  """
  awsSecretAccessKey: String!
  """
  The name of the S3 bucket.
  """
  bucket: String!
  """
  The S3 Data Source's tables.
  """
  tables: [S3DataSourceTableInput!]!
}

input CreateWebhookDataSourceInput {
  """
  The Webhook Data Source's connection settings
  """
  connectionSettings: WebhookConnectionSettingsInput!
  """
  The Webhook Data Source's description.
  """
  description: String
  """
  The Webhook Data Source's unique name. If not specified, Propel will set the ID as unique name.
  """
  uniqueName: String
}

input ModifyWebhookDataSourceInput {
  """
  The Webhook Data Source's new connection settings. If not provided this property will not be modified.
  """
  connectionSettings: PartialWebhookConnectionSettingsInput
  """
  The Webhook Data Source's new description. If not provided this property will not be modified.
  """
  description: String
  """
  The ID or unique name of the Webhook Data Source to modify.
  """
  idOrUniqueName: idOrUniqueName!
  """
  The Webhook Data Source's new unique name. If not provided this property will not be modified.
  """
  uniqueName: String
}

"""
The Webhook Data Source connection settings.
"""
input PartialWebhookConnectionSettingsInput {
  """
  The HTTP basic authentication settings for the Webhook Data Source URL. If this parameter is not provided, anyone with the webhook URL will be able to send events. While it's OK to test without HTTP Basic authentication, we recommend enabling it. If not provided this property will not be modified.
  """
  basicAuth: HttpBasicAuthInput
  """
  Set this to `false` to disable HTTP Basic authentication. Any previously stored HTTP Basic authentication settings will be cleared out. If not provided this property will not be modified.
  """
  basicAuthEnabled: Boolean
}

"""
The Webhook Data Source connection settings.
"""
type WebhookConnectionSettings {
  """
  The HTTP basic authentication settings for the Webhook Data Source URL. If this parameter is not provided, anyone with the webhook URL will be able to send events. While it's OK to test without HTTP Basic authentication, we recommend enabling it.
  """
  basicAuth: HttpBasicAuthSettings
  """
  The additional columns for the Webhook Data Source table.
  """
  columns: [WebhookDataSourceColumn!]
  """
  The tenant ID column, if configured.
  """
  tenant: String
  """
  The primary timestamp column.
  """
  timestamp: String!
  """
  The unique ID column. Propel uses the primary timestamp and a unique ID to compose a primary key for determining whether records should be inserted, deleted, or updated.
  """
  uniqueId: String
  """
  The Webhook URL for posting JSON events
  """
  webhookUrl: String!
}

"""
The Webhook Data Source connection settings.
"""
input WebhookConnectionSettingsInput {
  """
  The HTTP basic authentication settings for the Webhook Data Source URL. If this parameter is not provided, anyone with the webhook URL will be able to send events. While it's OK to test without HTTP Basic authentication, we recommend enabling it.
  """
  basicAuth: HttpBasicAuthInput
  """
  The additional columns for the Webhook Data Source table.
  """
  columns: [WebhookDataSourceColumnInput!]
  """
  The tenant ID column, if configured.
  """
  tenant: String
  """
  The primary timestamp column.
  """
  timestamp: String!
  """
  The unique ID column. Propel uses the primary timestamp and a unique ID to compose a primary key for determining whether records should be inserted, deleted, or updated.
  """
  uniqueId: String
}

"""
The fields for querying records by unique ID.
"""
input RecordsByUniqueIdInput {
  """
  Optionally specifies the Propeller to use. Applications may not set this value. Instead, Application Queries always use the Propeller configured on the Application.
  """
  propeller: Propeller
  """
  The Data Pool to be queried. A Data Pool ID or unique name can be provided.
  """
  dataPool: DataPoolInput!
  """
  The columns to retrieve.
  """
  columns: [String!]!
  """
  The unique IDs of the records to retrieve.
  """
  uniqueIds: [String!]!
}

type RecordsByUniqueIdResponse {
  """
  The Data Pool columns for the record.
  """
  columns: [String!]!
  """
  An array of values for the record.
  """
  values: [[String]!]!
  """
  The Query statistics and metadata.
  """
  query: QueryInfo!
}
